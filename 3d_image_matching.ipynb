{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":91498,"databundleVersionId":11655853,"sourceType":"competition"}],"dockerImageVersionId":30918,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 3D Scene Reconstruction with Outlier Detection (GPU Optimized Approach)\nIn this notebook, we'll develop a solution for reconstructing 3D scenes from image collections while identifying and filtering out unrelated images. Our approach involves detecting which images belong to the same scenes and which are outliers, followed by camera pose estimation for the images that belong togther.","metadata":{}},{"cell_type":"markdown","source":"## Setup and GPU Optimization","metadata":{}},{"cell_type":"code","source":"# Import necessary libraries\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport cv2\nfrom pathlib import Path\nimport networkx as nx\nfrom sklearn.cluster import DBSCAN\nfrom tqdm import tqdm\nimport warnings\nimport torch\nimport gc\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nwarnings.filterwarnings('ignore')\n\n# Set random seed for reproducibility\nnp.random.seed(42)\ntorch.manual_seed(42)\n\n# Define paths\nTRAIN_DIR = \"/kaggle/input/image-matching-challenge-2025/train/\"  # Path to training data\nTEST_DIR = \"/kaggle/input/image-matching-challenge-2025/test/\"    # Path to test data\nOUTPUT_FILE = \"submission.csv\"  # Path for output file\n\n# Check GPU availability and set up CUDA for GPU acceleration\nprint(\"CUDA available:\", torch.cuda.is_available())\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\n    torch.cuda.empty_cache()\n    torch.backends.cudnn.benchmark = True  # Optimize CUDA performance\n    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n    # Print GPU memory info\n    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\nelse:\n    device = torch.device(\"cpu\")\n    print(\"Using CPU\")\n\n# Enable OpenCV GPU acceleration if available\nif cv2.cuda.getCudaEnabledDeviceCount() > 0:\n    print(\"OpenCV CUDA acceleration enabled\")\nelse:\n    print(\"OpenCV CUDA acceleration not available\")\n\n# Memory management function\ndef free_memory():\n    \"\"\"Free GPU memory and call garbage collector.\"\"\"\n    torch.cuda.empty_cache()\n    gc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T03:10:25.039536Z","iopub.execute_input":"2025-04-03T03:10:25.039821Z","iopub.status.idle":"2025-04-03T03:10:30.338367Z","shell.execute_reply.started":"2025-04-03T03:10:25.039796Z","shell.execute_reply":"2025-04-03T03:10:30.337561Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Read Training Data and Define Parameters","metadata":{}},{"cell_type":"code","source":"# Read training labels and thresholds from CSV files\nprint(\"\\nReading training CSV files...\")\n\ndef read_training_csvs():\n    \"\"\"Read training CSV files with ground truth scene labels.\n    \n    Returns:\n        Dictionary with training CSV data\n    \"\"\"\n    csv_data = {\n        'labels': None,\n        'thresholds': None\n    }\n    \n    # Look for train_labels.csv\n    labels_path = '/kaggle/input/image-matching-challenge-2025/train_labels.csv'\n    if Path(labels_path).exists():\n        try:\n            labels_df = pd.read_csv(labels_path)\n            print(f\"Found train_labels.csv with {len(labels_df)} rows\")\n            csv_data['labels'] = labels_df\n        except Exception as e:\n            print(f\"Error reading train_labels.csv: {e}\")\n    else:\n        print(\"train_labels.csv not found\")\n        \n    # Look for train_thresholds.csv\n    thresholds_path = '/kaggle/input/image-matching-challenge-2025/train_thresholds.csv'\n    if Path(thresholds_path).exists():\n        try:\n            thresholds_df = pd.read_csv(thresholds_path)\n            print(f\"Found train_thresholds.csv with {len(thresholds_df)} rows\")\n            csv_data['thresholds'] = thresholds_df\n        except Exception as e:\n            print(f\"Error reading train_thresholds.csv: {e}\")\n    else:\n        print(\"train_thresholds.csv not found\")\n        \n    return csv_data\n\n# Read training CSVs\ntraining_csvs = read_training_csvs()\n\n# Set feature matching parameters - optimized based on ground truth data\nFEATURE_MATCHING_PARAMS = {\n    'ratio_test': 0.75,  # Lowe's ratio test threshold\n    'min_inliers': 15,   # Minimum number of inliers required\n    'inlier_ratio': 0.3  # Minimum ratio of inliers to matches\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T03:11:14.168642Z","iopub.execute_input":"2025-04-03T03:11:14.169222Z","iopub.status.idle":"2025-04-03T03:11:14.208808Z","shell.execute_reply.started":"2025-04-03T03:11:14.169188Z","shell.execute_reply":"2025-04-03T03:11:14.207976Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Analyze Training Data\n","metadata":{}},{"cell_type":"code","source":"# If we found the training CSV files, use them to extract ground truth information\nif training_csvs['labels'] is not None:\n    print(\"\\nExtracting ground truth information from training CSV files...\")\n    \n    # Extract ground truth scene assignments\n    labels_df = training_csvs['labels']\n    \n    # Get unique datasets and count their scenes\n    dataset_scenes = labels_df.groupby('dataset')['scene'].nunique()\n    \n    # Build training structure from CSV data\n    training_structure = {}\n    \n    for dataset, scene_count in dataset_scenes.items():\n        # Get dataset images\n        dataset_images = labels_df[labels_df['dataset'] == dataset]\n        \n        # Count images per scene\n        scene_counts = dataset_images.groupby('scene').size()\n        \n        # Check for outliers\n        has_outliers = 'outliers' in scene_counts.index\n        outlier_count = scene_counts.get('outliers', 0)\n        \n        # Calculate total images and outlier percentage\n        total_images = dataset_images.shape[0]\n        outlier_percentage = (outlier_count / total_images) * 100 if total_images > 0 else 0\n        \n        # Add to training structure\n        training_structure[dataset] = {\n            'path': Path(TRAIN_DIR) / dataset,\n            'expected_scene_count': scene_count - (1 if has_outliers else 0),  # Don't count outliers as a scene\n            'scene_counts': scene_counts.to_dict(),\n            'total_images': total_images,\n            'outlier_count': outlier_count,\n            'outlier_percentage': outlier_percentage\n        }\n    \n    # Print training structure information\n    print(\"\\nTraining Structure from CSV:\")\n    for dataset, info in training_structure.items():\n        scene_count = info['expected_scene_count']\n        print(f\"  {dataset}: {scene_count} scenes, {info['total_images']} images, {info['outlier_count']} outliers ({info['outlier_percentage']:.1f}%)\")\n    \n    # Calculate training statistics\n    scene_counts = [info['expected_scene_count'] for _, info in training_structure.items()]\n    image_per_scene = []\n    outlier_percentages = [info['outlier_percentage'] for _, info in training_structure.items()]\n    \n    # Calculate average images per scene (excluding outliers)\n    for dataset, info in training_structure.items():\n        scene_image_counts = {k: v for k, v in info['scene_counts'].items() if k != 'outliers'}\n        if scene_image_counts:\n            image_per_scene.extend(scene_image_counts.values())\n    \n    training_stats = {\n        'avg_scenes_per_dataset': np.mean(scene_counts) if scene_counts else 2.0,\n        'avg_images_per_scene': np.mean(image_per_scene) if image_per_scene else 15.0,\n        'avg_outlier_percentage': np.mean(outlier_percentages) if outlier_percentages else 10.0,\n        'outlier_datasets_percentage': sum(1 for info in training_structure.values() if info['outlier_count'] > 0) / len(training_structure) * 100 if training_structure else 50.0\n    }\n    \n    print(\"\\nTraining Statistics from CSV:\")\n    print(f\"  Average scenes per dataset: {training_stats['avg_scenes_per_dataset']:.2f}\")\n    print(f\"  Average images per scene: {training_stats['avg_images_per_scene']:.2f}\")\n    print(f\"  Average outlier percentage: {training_stats['avg_outlier_percentage']:.2f}%\")\n    print(f\"  Datasets with outliers: {training_stats['outlier_datasets_percentage']:.2f}%\")\n    \n    # Check if thresholds are available\n    if training_csvs['thresholds'] is not None:\n        thresholds_df = training_csvs['thresholds']\n        print(\"\\nThreshold information available for scenes\")\nelse:\n    print(\"No training CSV files found. Using default parameters.\")\n    # Use default parameters\n    training_structure = {}\n    training_stats = {\n        'avg_scenes_per_dataset': 2.31,  # Based on previous analysis\n        'avg_images_per_scene': 60.77,\n        'avg_outlier_percentage': 5.85,\n        'outlier_datasets_percentage': 30.77\n    }\n\nprint(\"\\nUsing Feature Matching Parameters:\")\nfor param, value in FEATURE_MATCHING_PARAMS.items():\n    print(f\"  {param}: {value}\")\n\n# Set up test dataset info\ntest_dataset_info = {}\nfor dataset_path in Path(TEST_DIR).glob('*'):\n    if dataset_path.is_dir():\n        dataset_name = dataset_path.name\n        \n        # If we have this dataset in training, use its information\n        if dataset_name in training_structure:\n            expected_scene_count = training_structure[dataset_name]['expected_scene_count']\n            print(f\"Using scene count from training for {dataset_name}: {expected_scene_count} scenes\")\n            \n            test_dataset_info[dataset_name] = {\n                'path': dataset_path,\n                'expected_scene_count': expected_scene_count\n            }\n        else:\n            # No matching training data, use the average\n            avg_scenes = int(round(training_stats['avg_scenes_per_dataset']))\n            test_dataset_info[dataset_name] = {\n                'path': dataset_path,\n                'expected_scene_count': avg_scenes\n            }\n            print(f\"No training data for {dataset_name}, using average: {avg_scenes} scenes\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T03:11:42.966997Z","iopub.execute_input":"2025-04-03T03:11:42.967328Z","iopub.status.idle":"2025-04-03T03:11:43.041304Z","shell.execute_reply.started":"2025-04-03T03:11:42.967298Z","shell.execute_reply":"2025-04-03T03:11:43.040443Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## GPU-Accelerated Feature Extraction and Matching","metadata":{}},{"cell_type":"code","source":"def extract_features_gpu(image_path):\n    \"\"\"Extract SIFT features from an image using GPU acceleration if available.\n    \n    Args:\n        image_path: Path to the image file\n        \n    Returns:\n        Tuple of (keypoints, descriptors, image_dimensions)\n    \"\"\"\n    # Read image\n    img = cv2.imread(str(image_path), cv2.IMREAD_GRAYSCALE)\n    if img is None:\n        return None, None, None\n    \n    # Use CUDA SIFT if available\n    if hasattr(cv2, 'cuda_SIFT'):\n        # Convert image to GPU\n        gpu_img = cv2.cuda_GpuMat()\n        gpu_img.upload(img)\n        \n        # Extract features\n        sift_gpu = cv2.cuda_SIFT.create(nfeatures=2000)\n        keypoints, descriptors = sift_gpu.detectAndCompute(gpu_img, None)\n    else:\n        # Use CPU SIFT\n        sift = cv2.SIFT_create(nfeatures=2000)\n        keypoints, descriptors = sift.detectAndCompute(img, None)\n    \n    return keypoints, descriptors, img.shape\n\ndef process_dataset_features(dataset_path, max_workers=4):\n    \"\"\"Process all images in a dataset and extract features using parallel processing.\n    \n    Args:\n        dataset_path: Path to the dataset directory\n        max_workers: Maximum number of parallel workers\n        \n    Returns:\n        Dictionary mapping image names to their features\n    \"\"\"\n    features_dict = {}\n    \n    # Get all image files (only .png for this dataset)\n    image_files = list(dataset_path.glob('*.png'))\n    \n    # Define a worker function for parallel processing\n    def process_image(img_path):\n        keypoints, descriptors, dimensions = extract_features_gpu(img_path)\n        if keypoints is not None and descriptors is not None and len(keypoints) > 0:\n            return img_path.name, {\n                'keypoints': keypoints,\n                'descriptors': descriptors,\n                'dimensions': dimensions\n            }\n        return None, None\n    \n    # Process images in parallel\n    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n        futures = [executor.submit(process_image, img_path) for img_path in image_files]\n        \n        for future in tqdm(as_completed(futures), total=len(futures), desc=\"Extracting features\"):\n            img_name, features = future.result()\n            if img_name is not None:\n                features_dict[img_name] = features\n                features_dict[img_name]['path'] = dataset_path / img_name\n    \n    # Free memory after feature extraction\n    free_memory()\n    return features_dict\n\ndef match_features_gpu(desc1, desc2, kp1, kp2, params=None):\n    \"\"\"Match features between two images using ratio test and geometric verification.\n    GPU accelerated when possible.\n    \n    Args:\n        desc1, desc2: Feature descriptors from both images\n        kp1, kp2: Keypoints from both images\n        params: Dictionary with matching parameters\n        \n    Returns:\n        Tuple of (matches, inlier_count, is_geometrically_consistent)\n    \"\"\"\n    if params is None:\n        params = FEATURE_MATCHING_PARAMS\n    \n    # Extract parameters\n    ratio = params['ratio_test']\n    min_inliers = params['min_inliers']\n    inlier_ratio = params['inlier_ratio']\n    \n    # Handle empty descriptors\n    if desc1 is None or desc2 is None or len(desc1) < 8 or len(desc2) < 8:\n        return [], 0, False\n    \n    try:\n        # Check if CUDA BFMatcher is available\n        if hasattr(cv2, 'cuda_DescriptorMatcher_createBFMatcher'):\n            # Upload descriptors to GPU\n            gpu_desc1 = cv2.cuda_GpuMat()\n            gpu_desc2 = cv2.cuda_GpuMat()\n            gpu_desc1.upload(np.ascontiguousarray(desc1, dtype=np.float32))\n            gpu_desc2.upload(np.ascontiguousarray(desc2, dtype=np.float32))\n            \n            # Create GPU matcher\n            matcher = cv2.cuda_DescriptorMatcher_createBFMatcher(cv2.NORM_L2)\n            gpu_matches = matcher.knnMatch(gpu_desc1, gpu_desc2, k=2)\n            \n            # Apply ratio test\n            good_matches = [m for m, n in gpu_matches if m.distance < ratio * n.distance]\n        else:\n            # Use FLANN matcher on CPU\n            FLANN_INDEX_KDTREE = 1\n            index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n            search_params = dict(checks=50)\n            \n            flann = cv2.FlannBasedMatcher(index_params, search_params)\n            matches = flann.knnMatch(desc1, desc2, k=2)\n            \n            # Apply ratio test to find good matches\n            good_matches = []\n            for i, pair in enumerate(matches):\n                if len(pair) == 2:  # Ensure we have 2 matches\n                    m, n = pair\n                    if m.distance < ratio * n.distance:\n                        good_matches.append(m)\n        \n        # Perform geometric verification if we have enough matches\n        is_consistent = False\n        inlier_count = 0\n        \n        if len(good_matches) >= 8:\n            # Extract matched keypoints\n            src_pts = np.float32([kp1[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n            dst_pts = np.float32([kp2[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n            \n            # Find fundamental matrix\n            F, mask = cv2.findFundamentalMat(src_pts, dst_pts, cv2.FM_RANSAC, 3.0)\n            \n            if F is not None and mask is not None:\n                inliers = mask.ravel() == 1\n                inlier_count = np.sum(inliers)\n                \n                # Consider match geometrically consistent if enough inliers\n                is_consistent = inlier_count >= max(min_inliers, inlier_ratio * len(good_matches))\n    \n    except Exception as e:\n        # Handle potential errors in feature matching\n        return [], 0, False\n        \n    return good_matches, inlier_count, is_consistent\n\ndef build_match_graph(features_dict, batch_size=100):\n    \"\"\"Build a graph representing image matches using batched processing.\n    \n    Args:\n        features_dict: Dictionary of image features\n        batch_size: Number of edges to process in each batch\n        \n    Returns:\n        NetworkX graph where nodes are images and edges represent matches\n    \"\"\"\n    # Create graph\n    G = nx.Graph()\n    \n    # Add nodes for each image\n    for img_name in features_dict.keys():\n        G.add_node(img_name)\n    \n    # Create all image pairs for matching\n    image_names = list(features_dict.keys())\n    n = len(image_names)\n    \n    if n <= 1:  # Only proceed if we have at least 2 images\n        return G\n    \n    # Create all pairs for processing\n    pairs = [(i, j) for i in range(n) for j in range(i+1, n)]\n    total_pairs = len(pairs)\n    \n    # Process in batches to avoid memory issues\n    for batch_start in tqdm(range(0, total_pairs, batch_size), desc=\"Building match graph\"):\n        batch_end = min(batch_start + batch_size, total_pairs)\n        batch_pairs = pairs[batch_start:batch_end]\n        \n        for i, j in batch_pairs:\n            img1_name = image_names[i]\n            img2_name = image_names[j]\n            \n            kp1 = features_dict[img1_name]['keypoints']\n            desc1 = features_dict[img1_name]['descriptors']\n            kp2 = features_dict[img2_name]['keypoints']\n            desc2 = features_dict[img2_name]['descriptors']\n            \n            # Match features and verify\n            _, inlier_count, is_consistent = match_features_gpu(desc1, desc2, kp1, kp2)\n            \n            # Add edge if geometrically consistent\n            if is_consistent:\n                G.add_edge(img1_name, img2_name, weight=inlier_count)\n        \n        # Free memory after each batch\n        if batch_end % (batch_size * 10) == 0:\n            free_memory()\n    \n    return G","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T03:12:15.511837Z","iopub.execute_input":"2025-04-03T03:12:15.512230Z","iopub.status.idle":"2025-04-03T03:12:15.528669Z","shell.execute_reply.started":"2025-04-03T03:12:15.512199Z","shell.execute_reply":"2025-04-03T03:12:15.527699Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Efficient Scene Clustering","metadata":{}},{"cell_type":"code","source":"def cluster_scenes(graph, expected_scene_count=None):\n    \"\"\"Cluster images into scenes based on the match graph.\n    \n    Args:\n        graph: NetworkX graph of image matches\n        expected_scene_count: Optional expected number of scenes\n        \n    Returns:\n        Tuple of (scenes, outliers) where:\n            scenes: List of lists, each containing image names in a scene\n            outliers: List of image names identified as outliers\n    \"\"\"\n    # Get graph nodes\n    image_names = list(graph.nodes())\n    \n    # If graph is empty or has only one node, return early\n    if len(image_names) <= 1:\n        if len(image_names) == 1:\n            return [], image_names  # Single image is an outlier\n        else:\n            return [], []  # Empty graph\n    \n    # If graph has no edges, all images are outliers\n    if not graph.edges():\n        return [], image_names\n    \n    # Analyze graph connectivity\n    connected_components = list(nx.connected_components(graph))\n    \n    # If we have multiple connected components, use them as initial clusters\n    if len(connected_components) > 1:\n        scenes = []\n        outliers = []\n        \n        for component in connected_components:\n            component_list = list(component)\n            if len(component_list) >= 3:\n                # This is a potential scene\n                scenes.append(component_list)\n            else:\n                # Too small to be a scene\n                outliers.extend(component_list)\n                \n        return scenes, outliers\n    \n    # Get graph adjacency matrix for clustering\n    adj_matrix = nx.to_numpy_array(graph)\n    \n    # Use spectral clustering if we have expected scene count and more than one scene\n    if expected_scene_count is not None and expected_scene_count > 1:\n        try:\n            from sklearn.cluster import SpectralClustering\n            \n            # Create affinity matrix from adjacency (higher weight = higher affinity)\n            max_weight = np.max(adj_matrix) if np.max(adj_matrix) > 0 else 1\n            affinity_matrix = adj_matrix / max_weight\n            \n            # Apply spectral clustering\n            clustering = SpectralClustering(\n                n_clusters=expected_scene_count,\n                affinity='precomputed',\n                random_state=42\n            )\n            \n            labels = clustering.fit_predict(affinity_matrix)\n        except Exception:\n            # Fall back to DBSCAN\n            expected_scene_count = None\n    \n    # Use DBSCAN if no expected scene count or spectral clustering failed\n    if expected_scene_count is None:\n        # Convert to distance matrix (higher weight = lower distance)\n        max_weight = np.max(adj_matrix) if np.max(adj_matrix) > 0 else 1\n        dist_matrix = 1 - (adj_matrix / max_weight)\n        np.fill_diagonal(dist_matrix, 0)  # Zero distance to self\n        \n        # Apply DBSCAN with best parameters from previous analysis\n        clustering = DBSCAN(eps=0.6, min_samples=3, metric='precomputed')\n        labels = clustering.fit_predict(dist_matrix)\n    \n    # Group images by cluster\n    scenes = []\n    outliers = []\n    \n    # Process each unique label\n    unique_labels = set(labels)\n    for label in unique_labels:\n        if label == -1:  # DBSCAN outliers (or not assigned in spectral clustering)\n            outlier_indices = np.where(labels == -1)[0]\n            outliers.extend([image_names[i] for i in outlier_indices])\n        else:\n            # Get cluster members\n            cluster_indices = np.where(labels == label)[0]\n            cluster_names = [image_names[i] for i in cluster_indices]\n            \n            # Check if the cluster forms a connected component in the graph\n            subgraph = graph.subgraph(cluster_names)\n            \n            if len(cluster_names) >= 3:  # Minimum cluster size\n                if nx.is_connected(subgraph):\n                    scenes.append(cluster_names)\n                else:\n                    # If not connected, split into connected components\n                    sub_components = list(nx.connected_components(subgraph))\n                    for component in sub_components:\n                        comp_list = list(component)\n                        if len(comp_list) >= 3:\n                            scenes.append(comp_list)\n                        else:\n                            outliers.extend(comp_list)\n            else:\n                # If too small, treat as outliers\n                outliers.extend(cluster_names)\n    \n    # Add disconnected nodes (not in any edge) to outliers\n    connected_nodes = set()\n    for edge in graph.edges():\n        connected_nodes.add(edge[0])\n        connected_nodes.add(edge[1])\n    \n    disconnected = set(graph.nodes()) - connected_nodes\n    outliers.extend(disconnected)\n    \n    return scenes, outliers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T03:12:40.592555Z","iopub.execute_input":"2025-04-03T03:12:40.592881Z","iopub.status.idle":"2025-04-03T03:12:40.603471Z","shell.execute_reply.started":"2025-04-03T03:12:40.592852Z","shell.execute_reply":"2025-04-03T03:12:40.602538Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Optimized Camera Pose Estimation","metadata":{}},{"cell_type":"code","source":"def estimate_poses(scene_images, features_dict):\n    \"\"\"Estimate camera poses for a scene using Structure from Motion approach.\n    \n    Args:\n        scene_images: List of image names in the scene\n        features_dict: Dictionary of image features\n        \n    Returns:\n        Dictionary mapping image names to (R, T) pairs\n    \"\"\"\n    # Initialize poses dictionary\n    poses = {}\n    \n    # Set first camera as reference (identity rotation, zero translation)\n    if scene_images:\n        poses[scene_images[0]] = (np.eye(3), np.zeros(3))\n    \n    # If only one image, we're done\n    if len(scene_images) == 1:\n        return poses\n    \n    # Build a spanning tree of matches\n    G = nx.Graph()\n    for img_name in scene_images:\n        G.add_node(img_name)\n    \n    # Add edges with weights based on match quality\n    for i, img1 in enumerate(scene_images):\n        kp1 = features_dict[img1]['keypoints']\n        desc1 = features_dict[img1]['descriptors']\n        \n        for j in range(i+1, len(scene_images)):\n            img2 = scene_images[j]\n            kp2 = features_dict[img2]['keypoints']\n            desc2 = features_dict[img2]['descriptors']\n            \n            _, inlier_count, is_consistent = match_features_gpu(desc1, desc2, kp1, kp2)\n            \n            if is_consistent:\n                G.add_edge(img1, img2, weight=inlier_count)\n    \n    # Find maximum spanning tree to avoid loops\n    if len(G.edges()) > 0:\n        try:\n            mst = nx.maximum_spanning_tree(G, weight='weight')\n            \n            # Add remaining images using breadth-first search from reference camera\n            queue = [scene_images[0]]\n            visited = {scene_images[0]}\n            \n            while queue and len(poses) < len(scene_images):\n                current = queue.pop(0)\n                \n                # Process neighbors\n                for neighbor in mst.neighbors(current):\n                    if neighbor not in visited:\n                        # Get feature matches\n                        kp1 = features_dict[current]['keypoints']\n                        desc1 = features_dict[current]['descriptors']\n                        kp2 = features_dict[neighbor]['keypoints']\n                        desc2 = features_dict[neighbor]['descriptors']\n                        \n                        matches, _, _ = match_features_gpu(desc1, desc2, kp1, kp2)\n                        \n                        if len(matches) >= 8:\n                            # Extract matched points\n                            pts1 = np.float32([kp1[m.queryIdx].pt for m in matches])\n                            pts2 = np.float32([kp2[m.trainIdx].pt for m in matches])\n                            \n                            # Estimate essential matrix\n                            focal = max(features_dict[current]['dimensions']) / 2\n                            pp = (features_dict[current]['dimensions'][1] / 2, \n                                features_dict[current]['dimensions'][0] / 2)\n                            \n                            E, mask = cv2.findEssentialMat(pts1, pts2, focal=focal, pp=pp, method=cv2.FM_RANSAC)\n                            \n                            if E is not None:\n                                # Recover relative pose\n                                _, R, t, _ = cv2.recoverPose(E, pts1, pts2, focal=focal, pp=pp)\n                                \n                                # Get current pose\n                                R_current, t_current = poses[current]\n                                \n                                # Calculate neighbor pose based on relative pose\n                                R_neighbor = R_current @ R.T\n                                t_neighbor = t_current - R_neighbor @ t.ravel()\n                                \n                                # Store pose\n                                poses[neighbor] = (R_neighbor, t_neighbor)\n                        \n                        # Mark as visited and add to queue\n                        visited.add(neighbor)\n                        queue.append(neighbor)\n        except Exception as e:\n            pass\n    \n    # For images without poses (unregistered), add them but mark with None\n    for img_name in scene_images:\n        if img_name not in poses:\n            poses[img_name] = (None, None)\n    \n    return poses","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T03:13:00.045005Z","iopub.execute_input":"2025-04-03T03:13:00.045323Z","iopub.status.idle":"2025-04-03T03:13:00.056160Z","shell.execute_reply.started":"2025-04-03T03:13:00.045297Z","shell.execute_reply":"2025-04-03T03:13:00.055441Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Process Test Datasets with Memory Management","metadata":{}},{"cell_type":"code","source":"def format_matrix_for_submission(matrix):\n    \"\"\"Format a matrix as a semicolon-separated string.\"\"\"\n    if matrix is None:\n        return \";\".join([\"nan\"] * 9)\n    return \";\".join([str(float(x)) for x in matrix.flatten()])\n\ndef format_vector_for_submission(vector):\n    \"\"\"Format a vector as a semicolon-separated string.\"\"\"\n    if vector is None:\n        return \";\".join([\"nan\"] * 3)\n    return \";\".join([str(float(x)) for x in vector])\n\ndef process_test_dataset(dataset_path, dataset_name, training_structure, test_dataset_info):\n    \"\"\"Process a test dataset to identify scenes and estimate poses.\n    \n    Args:\n        dataset_path: Path to test dataset\n        dataset_name: Name of the dataset\n        training_structure: Structure information from training data\n        test_dataset_info: Additional analysis info for test datasets\n        \n    Returns:\n        Dictionary with scenes, outliers, and poses\n    \"\"\"\n    print(f\"  Processing dataset '{dataset_name}'...\")\n    \n    # Extract features from test dataset\n    features_dict = process_dataset_features(dataset_path)\n    \n    if not features_dict:\n        print(f\"  Warning: No features extracted from {dataset_name}\")\n        return {\n            'scenes': [],\n            'outliers': [img.name for img in dataset_path.glob('*.png')],\n            'poses': {}\n        }\n    \n    print(f\"  Extracted features from {len(features_dict)} images\")\n    \n    # Build match graph\n    match_graph = build_match_graph(features_dict)\n    \n    print(f\"  Match graph has {match_graph.number_of_nodes()} nodes and {match_graph.number_of_edges()} edges\")\n    \n    # Get expected scene count from test_dataset_info if available\n    expected_scene_count = None\n    if dataset_name in test_dataset_info:\n        expected_scene_count = test_dataset_info[dataset_name].get('expected_scene_count')\n        print(f\"  Using expected scene count from analysis: {expected_scene_count}\")\n    \n    # Cluster into scenes\n    print(f\"  Clustering scenes...\")\n    scenes, outliers = cluster_scenes(match_graph, expected_scene_count)\n    \n    print(f\"  Identified {len(scenes)} scenes and {len(outliers)} outliers\")\n    \n    # Estimate poses for each scene\n    all_poses = {}\n    \n    for i, scene in enumerate(scenes):\n        print(f\"  Estimating poses for scene {i+1} ({len(scene)} images)...\")\n        scene_poses = estimate_poses(scene, features_dict)\n        \n        # Count registered images\n        registered_count = sum(1 for _, (R, T) in scene_poses.items() if R is not None and T is not None)\n        print(f\"  Registered {registered_count}/{len(scene)} images in scene {i+1}\")\n        \n        all_poses.update(scene_poses)\n        \n        # Free memory\n        free_memory()\n    \n    # Help garbage collection\n    del features_dict\n    del match_graph\n    free_memory()\n    \n    return {\n        'scenes': scenes,\n        'outliers': outliers,\n        'poses': all_poses\n    }\n\ndef create_submission_file(test_dir, output_file, training_structure, test_dataset_info):\n    \"\"\"Process all test datasets and create submission file.\n    \n    Args:\n        test_dir: Base directory containing all test datasets\n        output_file: Path to output submission file\n        training_structure: Structure information from training data\n        test_dataset_info: Additional analysis info for test datasets\n        \n    Returns:\n        Dataframe with submission results\n    \"\"\"\n    # Initialize results list\n    results = []\n    \n    # Check if test directory exists\n    test_path = Path(test_dir)\n    if not test_path.exists():\n        print(f\"Error: Test directory not found: {test_dir}\")\n        return pd.DataFrame()\n    \n    # Get list of test datasets\n    test_datasets = [p for p in test_path.glob('*') if p.is_dir()]\n    if not test_datasets:\n        print(f\"Error: No datasets found in {test_dir}\")\n        return pd.DataFrame()\n    \n    print(f\"\\nFound {len(test_datasets)} test datasets\")\n    \n    # Process each test dataset\n    for dataset_path in test_datasets:\n        dataset_name = dataset_path.name\n        print(f\"\\nProcessing test dataset: {dataset_name}\")\n        \n        # Process dataset\n        dataset_results = process_test_dataset(\n            dataset_path, \n            dataset_name, \n            training_structure,\n            test_dataset_info\n        )\n        \n        # Add results to list\n        # 1. Process scenes\n        for scene_idx, scene in enumerate(dataset_results['scenes']):\n            for img_name in scene:\n                R, T = dataset_results['poses'].get(img_name, (None, None))\n                \n                results.append({\n                    'dataset': dataset_name,\n                    'scene': f\"cluster{scene_idx+1}\",\n                    'image': img_name,\n                    'rotation_matrix': format_matrix_for_submission(R),\n                    'translation_vector': format_vector_for_submission(T)\n                })\n        \n        # 2. Process outliers\n        for img_name in dataset_results['outliers']:\n            results.append({\n                'dataset': dataset_name,\n                'scene': \"outliers\",\n                'image': img_name,\n                'rotation_matrix': \";\".join([\"nan\"] * 9),\n                'translation_vector': \";\".join([\"nan\"] * 3)\n            })\n        \n        print(f\"  Added {len(dataset_results['scenes'])} scenes and {len(dataset_results['outliers'])} outliers\")\n        \n        # Clear memory after each dataset\n        free_memory()\n    \n    # Convert to dataframe - do this in batches if the dataset is large\n    if results:\n        # Create dataframe in smaller chunks to save memory\n        batch_size = 1000\n        dfs = []\n        for i in range(0, len(results), batch_size):\n            batch = results[i:i+batch_size]\n            dfs.append(pd.DataFrame(batch))\n            \n        results_df = pd.concat(dfs, ignore_index=True)\n        del dfs  # Free memory\n        free_memory()\n    else:\n        results_df = pd.DataFrame(columns=[\n            'dataset', 'scene', 'image', 'rotation_matrix', 'translation_vector'\n        ])\n    \n    # Ensure all test images are included in the submission\n    all_test_images = {}\n    for dataset_path in test_datasets:\n        dataset_name = dataset_path.name\n        image_files = [img.name for img in dataset_path.glob('*.png')]\n        all_test_images[dataset_name] = set(image_files)\n    \n    # Check for missing images\n    missing_images = []\n    for dataset_name, images in all_test_images.items():\n        if dataset_name in results_df['dataset'].values:\n            submitted_images = set(results_df[results_df['dataset'] == dataset_name]['image'])\n            missing = images - submitted_images\n            if missing:\n                print(f\"Warning: Missing {len(missing)} images from dataset {dataset_name}\")\n                for img_name in missing:\n                    missing_images.append({\n                        'dataset': dataset_name,\n                        'scene': \"outliers\",  # Assume missing images are outliers\n                        'image': img_name,\n                        'rotation_matrix': \";\".join([\"nan\"] * 9),\n                        'translation_vector': \";\".join([\"nan\"] * 3)\n                    })\n        else:\n            # No results for this dataset at all\n            print(f\"Warning: No results generated for dataset {dataset_name}\")\n            for img_name in images:\n                missing_images.append({\n                    'dataset': dataset_name,\n                    'scene': \"outliers\",  # Assume all are outliers\n                    'image': img_name,\n                    'rotation_matrix': \";\".join([\"nan\"] * 9),\n                    'translation_vector': \";\".join([\"nan\"] * 3)\n                })\n    \n    # Add any missing images to results\n    if missing_images:\n        missing_df = pd.DataFrame(missing_images)\n        if results_df.empty:\n            results_df = missing_df\n        else:\n            results_df = pd.concat([results_df, missing_df], ignore_index=True)\n        del missing_df  # Free memory\n        free_memory()\n    \n    # Make sure we have all required columns\n    if not results_df.empty:\n        # Add image_id column\n        results_df.insert(0, 'image_id', range(len(results_df)))\n        \n        # Save to CSV\n        results_df.to_csv(output_file, index=False)\n        print(f\"\\nSubmission file created: {output_file}\")\n        print(f\"Total entries: {len(results_df)}\")\n    else:\n        print(\"Error: No results generated. Submission file not created.\")\n    \n    return results_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T03:13:34.981104Z","iopub.execute_input":"2025-04-03T03:13:34.981524Z","iopub.status.idle":"2025-04-03T03:13:35.003395Z","shell.execute_reply.started":"2025-04-03T03:13:34.981487Z","shell.execute_reply":"2025-04-03T03:13:35.002539Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Generate Submission File","metadata":{}},{"cell_type":"code","source":"# Execute the complete pipeline\nsubmission_df = create_submission_file(TEST_DIR, OUTPUT_FILE, training_structure, test_dataset_info)\n\n# Display a sample of the submission file\nif not submission_df.empty:\n    print(\"\\nSubmission sample:\")\n    print(submission_df.head())\n    \n    # Verify the structure\n    print(\"\\nSubmission structure validation:\")\n    print(f\"Total rows: {len(submission_df)}\")\n    print(f\"Unique datasets: {submission_df['dataset'].nunique()}\")\n    print(f\"Datasets: {', '.join(submission_df['dataset'].unique())}\")\n    \n    # Check for required pattern in scene names\n    scene_pattern_valid = all(s == 'outliers' or s.startswith('cluster') for s in submission_df['scene'].unique())\n    print(f\"Scene naming format valid: {scene_pattern_valid}\")\n    \n    # Count scenes and outliers\n    scene_counts = {}\n    outlier_counts = {}\n    \n    for dataset in submission_df['dataset'].unique():\n        dataset_df = submission_df[submission_df['dataset'] == dataset]\n        \n        # Count unique scenes (excluding outliers)\n        scenes = dataset_df[dataset_df['scene'] != 'outliers']['scene'].unique()\n        scene_counts[dataset] = len(scenes)\n        \n        # Count outliers\n        outlier_counts[dataset] = len(dataset_df[dataset_df['scene'] == 'outliers'])\n    \n    print(\"\\nScene and outlier counts:\")\n    for dataset, scene_count in scene_counts.items():\n        print(f\"  {dataset}: {scene_count} scenes, {outlier_counts[dataset]} outliers\")\nelse:\n    print(\"\\nError: Submission file generation failed\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T03:13:56.734212Z","iopub.execute_input":"2025-04-03T03:13:56.734558Z","iopub.status.idle":"2025-04-03T03:15:17.476488Z","shell.execute_reply.started":"2025-04-03T03:13:56.734529Z","shell.execute_reply":"2025-04-03T03:15:17.475621Z"}},"outputs":[],"execution_count":null}]}