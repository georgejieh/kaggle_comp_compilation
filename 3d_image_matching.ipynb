{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":91498,"databundleVersionId":11655853,"sourceType":"competition"}],"dockerImageVersionId":30918,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 3D Scene Reconstruction with Outlier Detection (Deep Learning Approach)\nIn this notebook, we'll develop a solution for reconstructing 3D scenes from image collections while identifying and filtering out unrelated images. Our approach involves detecting which images belong to the same scenes and which are outliers, followed by camera pose estimation for the images that belong togther.","metadata":{}},{"cell_type":"markdown","source":"## Setup and Configuration","metadata":{}},{"cell_type":"code","source":"# Import necessary libraries\nimport os\nimport sys\nimport gc\nimport warnings\nimport traceback\nfrom pathlib import Path\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\n\n# Third-party imports\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport cv2\nimport networkx as nx\nfrom sklearn.cluster import DBSCAN, SpectralClustering\nfrom tqdm import tqdm\nfrom scipy.sparse import lil_matrix\nfrom scipy.optimize import least_squares\n\n# Torch imports\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# Check for torch_geometric\ntry:\n    import torch_geometric\n    from torch_geometric.nn import GCNConv\n    from torch_geometric.data import Data\n    HAS_TORCH_GEOMETRIC = True\n    print(f\"torch_geometric is available (version {torch_geometric.__version__})\")\nexcept ImportError:\n    HAS_TORCH_GEOMETRIC = False\n    print(\"torch_geometric is not available, will use fallback clustering methods\")\n\n# Set random seed for reproducibility\nnp.random.seed(42)\ntorch.manual_seed(42)\n\n# Define paths\nTRAIN_DIR = \"/kaggle/input/image-matching-challenge-2025/train/\"  # Path to training data\nTEST_DIR = \"/kaggle/input/image-matching-challenge-2025/test/\"    # Path to test data\nOUTPUT_FILE = \"submission.csv\"  # Path for output file\n\n# LightGlue model paths\nLIGHTGLUE_DISK_PATH = \"/kaggle/input/lightglue/pytorch/disk/1/disk_lightglue.pth\"\nLIGHTGLUE_SIFT_PATH = \"/kaggle/input/lightglue/pytorch/sift/1/sift_lightglue.pth\"\n\n# SuperGlue/SuperPoint path\nSUPERGLUE_DIR = \"/kaggle/working/superglue_models\"\n\n# Suppress warnings\nwarnings.filterwarnings('ignore')\n\n# Check GPU availability and set up CUDA for GPU acceleration\nprint(\"CUDA available:\", torch.cuda.is_available())\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\n    torch.cuda.empty_cache()\n    torch.backends.cudnn.benchmark = True  # Optimize CUDA performance\n    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n    # Print GPU memory info\n    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\nelse:\n    device = torch.device(\"cpu\")\n    print(\"Using CPU\")\n\n# Memory management function\ndef free_memory():\n    \"\"\"Free GPU memory and call garbage collector.\"\"\"\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n    gc.collect()\n\n# Define parameters for deep learning-based feature extraction\nFEATURE_PARAMS = {\n    'superpoint': {\n        'weights': 'superpoint',  # Use pretrained weights\n        'nms_radius': 4,          # Non-maximum suppression radius\n        'max_keypoints': 4000,    # Maximum number of keypoints to detect\n        'keypoint_threshold': 0.005,  # Keypoint confidence threshold\n    },\n    'superglue': {\n        'weights': 'outdoor',     # Pretrained weights for outdoor scenes\n        'sinkhorn_iterations': 20, # Number of Sinkhorn iterations\n        'match_threshold': 0.2,   # Matching threshold\n        'ratio_test': 0.7,        # Ratio test threshold for traditional matching\n    }\n}\n\n# Parameters for pose estimation and verification\nMATCHING_PARAMS = {\n    'ransac_threshold': 0.5,     # RANSAC threshold (pixels)\n    'min_inliers': 20,           # Minimum number of inliers\n    'min_inlier_ratio': 0.4,     # Minimum inlier ratio\n    'geometric_verification': True  # Use multi-stage geometric verification\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T05:52:22.416634Z","iopub.execute_input":"2025-04-04T05:52:22.416868Z","iopub.status.idle":"2025-04-04T05:52:29.632342Z","shell.execute_reply.started":"2025-04-04T05:52:22.416845Z","shell.execute_reply":"2025-04-04T05:52:29.631544Z"}},"outputs":[{"name":"stdout","text":"torch_geometric is not available, will use fallback clustering methods\nCUDA available: True\nUsing GPU: Tesla T4\nGPU Memory: 15.83 GB\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## Setup SuperGlue/Superpoint Models","metadata":{}},{"cell_type":"code","source":"# Create directory for SuperGlue/SuperPoint models (if not using LightGlue)\nos.makedirs(SUPERGLUE_DIR, exist_ok=True)\n\n# Check if SuperGlue/SuperPoint model files already exist\nsuperpoint_exists = os.path.exists(f\"{SUPERGLUE_DIR}/superpoint_v1.pth\")\nsuperglue_exists = os.path.exists(f\"{SUPERGLUE_DIR}/superglue_outdoor.pth\")\nsuperpoint_py_exists = os.path.exists(f\"{SUPERGLUE_DIR}/superpoint.py\")\nsuperglue_py_exists = os.path.exists(f\"{SUPERGLUE_DIR}/superglue.py\")\ninit_py_exists = os.path.exists(f\"{SUPERGLUE_DIR}/__init__.py\")\n\n# Download model files if needed\nif not superpoint_exists:\n    print(\"Downloading SuperPoint model...\")\n    !wget -q -O {SUPERGLUE_DIR}/superpoint_v1.pth https://github.com/magicleap/SuperGluePretrainedNetwork/raw/master/models/weights/superpoint_v1.pth\n\nif not superglue_exists:\n    print(\"Downloading SuperGlue model...\")\n    !wget -q -O {SUPERGLUE_DIR}/superglue_outdoor.pth https://github.com/magicleap/SuperGluePretrainedNetwork/raw/master/models/weights/superglue_outdoor.pth\n\n# Download Python modules if needed\nif not superpoint_py_exists:\n    print(\"Downloading SuperPoint Python module...\")\n    !wget -q -O {SUPERGLUE_DIR}/superpoint.py https://raw.githubusercontent.com/magicleap/SuperGluePretrainedNetwork/master/models/superpoint.py\n\nif not superglue_py_exists:\n    print(\"Downloading SuperGlue Python module...\")\n    !wget -q -O {SUPERGLUE_DIR}/superglue.py https://raw.githubusercontent.com/magicleap/SuperGluePretrainedNetwork/master/models/superglue.py\n\nif not init_py_exists:\n    print(\"Downloading __init__.py module...\")\n    !wget -q -O {SUPERGLUE_DIR}/__init__.py https://raw.githubusercontent.com/magicleap/SuperGluePretrainedNetwork/master/models/__init__.py\n\n# Add SuperGlue directory to Python path\nsys.path.append(SUPERGLUE_DIR)\n\n# Verify SuperGlue/SuperPoint setup\nSUPERGLUE_AVAILABLE = (\n    os.path.exists(f\"{SUPERGLUE_DIR}/superpoint.py\") and \n    os.path.exists(f\"{SUPERGLUE_DIR}/superpoint_v1.pth\")\n)\n\nif SUPERGLUE_AVAILABLE:\n    print(\"SuperPoint and SuperGlue files are available\")\nelse:\n    print(\"Warning: SuperPoint and SuperGlue files are not available\")\n\n# Verify LightGlue models are available\nLIGHTGLUE_DISK_AVAILABLE = os.path.exists(LIGHTGLUE_DISK_PATH)\nLIGHTGLUE_SIFT_AVAILABLE = os.path.exists(LIGHTGLUE_SIFT_PATH)\n\nif LIGHTGLUE_DISK_AVAILABLE:\n    print(\"LightGlue DISK model is available\")\nelse:\n    print(\"Warning: LightGlue DISK model is not available at the expected path\")\n\nif LIGHTGLUE_SIFT_AVAILABLE:\n    print(\"LightGlue SIFT model is available\")\nelse:\n    print(\"Warning: LightGlue SIFT model is not available at the expected path\")\n\n# Check for Kornia (required for LightGlue)\ntry:\n    import kornia\n    KORNIA_AVAILABLE = True\n    print(f\"Kornia is available (version {kornia.__version__})\")\nexcept ImportError:\n    KORNIA_AVAILABLE = False\n    print(\"Warning: Kornia is not available, LightGlue will not work\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T05:53:07.007823Z","iopub.execute_input":"2025-04-04T05:53:07.008147Z","iopub.status.idle":"2025-04-04T05:53:12.211838Z","shell.execute_reply.started":"2025-04-04T05:53:07.008120Z","shell.execute_reply":"2025-04-04T05:53:12.210918Z"}},"outputs":[{"name":"stdout","text":"Downloading SuperPoint model...\nDownloading SuperGlue model...\nDownloading SuperPoint Python module...\nDownloading SuperGlue Python module...\nDownloading __init__.py module...\nSuperPoint and SuperGlue files are available\nLightGlue DISK model is available\nLightGlue SIFT model is available\nKornia is available (version 0.8.0)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## Read Training CSV Files and Extract Statistics","metadata":{}},{"cell_type":"code","source":"# Read training labels and thresholds from CSV files\nprint(\"\\nReading training CSV files...\")\n\ndef read_training_csvs():\n    \"\"\"Read training CSV files with ground truth scene labels.\n    \n    Returns:\n        Dictionary with training CSV data\n    \"\"\"\n    csv_data = {\n        'labels': None,\n        'thresholds': None\n    }\n    \n    # Look for train_labels.csv\n    labels_path = '/kaggle/input/image-matching-challenge-2025/train_labels.csv'\n    if Path(labels_path).exists():\n        try:\n            labels_df = pd.read_csv(labels_path)\n            print(f\"Found train_labels.csv with {len(labels_df)} rows\")\n            csv_data['labels'] = labels_df\n        except Exception as e:\n            print(f\"Error reading train_labels.csv: {e}\")\n    else:\n        print(\"train_labels.csv not found\")\n        \n    # Look for train_thresholds.csv\n    thresholds_path = '/kaggle/input/image-matching-challenge-2025/train_thresholds.csv'\n    if Path(thresholds_path).exists():\n        try:\n            thresholds_df = pd.read_csv(thresholds_path)\n            print(f\"Found train_thresholds.csv with {len(thresholds_df)} rows\")\n            csv_data['thresholds'] = thresholds_df\n        except Exception as e:\n            print(f\"Error reading train_thresholds.csv: {e}\")\n    else:\n        print(\"train_thresholds.csv not found\")\n        \n    return csv_data\n\n# Read training CSVs\ntraining_csvs = read_training_csvs()\n\n# Extract training statistics\nif training_csvs['labels'] is not None:\n    print(\"\\nExtracting ground truth information from training CSV files...\")\n    \n    # Extract ground truth scene assignments\n    labels_df = training_csvs['labels']\n    \n    # Get unique datasets and count their scenes\n    dataset_scenes = labels_df.groupby('dataset')['scene'].nunique()\n    \n    # Build training structure from CSV data\n    training_structure = {}\n    \n    for dataset, scene_count in dataset_scenes.items():\n        # Get dataset images\n        dataset_images = labels_df[labels_df['dataset'] == dataset]\n        \n        # Count images per scene\n        scene_counts = dataset_images.groupby('scene').size()\n        \n        # Check for outliers\n        has_outliers = 'outliers' in scene_counts.index\n        outlier_count = scene_counts.get('outliers', 0)\n        \n        # Calculate total images and outlier percentage\n        total_images = dataset_images.shape[0]\n        outlier_percentage = (outlier_count / total_images) * 100 if total_images > 0 else 0\n        \n        # Add to training structure\n        training_structure[dataset] = {\n            'path': Path(TRAIN_DIR) / dataset,\n            'expected_scene_count': scene_count - (1 if has_outliers else 0),  # Don't count outliers as a scene\n            'scene_counts': scene_counts.to_dict(),\n            'total_images': total_images,\n            'outlier_count': outlier_count,\n            'outlier_percentage': outlier_percentage\n        }\n    \n    # Print training structure information\n    print(\"\\nTraining Structure from CSV:\")\n    for dataset, info in training_structure.items():\n        scene_count = info['expected_scene_count']\n        print(f\"  {dataset}: {scene_count} scenes, {info['total_images']} images, {info['outlier_count']} outliers ({info['outlier_percentage']:.1f}%)\")\n    \n    # Calculate training statistics\n    scene_counts = [info['expected_scene_count'] for _, info in training_structure.items()]\n    image_per_scene = []\n    outlier_percentages = [info['outlier_percentage'] for _, info in training_structure.items()]\n    \n    # Calculate average images per scene (excluding outliers)\n    for dataset, info in training_structure.items():\n        scene_image_counts = {k: v for k, v in info['scene_counts'].items() if k != 'outliers'}\n        if scene_image_counts:\n            image_per_scene.extend(scene_image_counts.values())\n    \n    training_stats = {\n        'avg_scenes_per_dataset': np.mean(scene_counts) if scene_counts else 2.0,\n        'avg_images_per_scene': np.mean(image_per_scene) if image_per_scene else 15.0,\n        'avg_outlier_percentage': np.mean(outlier_percentages) if outlier_percentages else 10.0,\n        'outlier_datasets_percentage': sum(1 for info in training_structure.values() if info['outlier_count'] > 0) / len(training_structure) * 100 if training_structure else 50.0\n    }\n    \n    print(\"\\nTraining Statistics from CSV:\")\n    print(f\"  Average scenes per dataset: {training_stats['avg_scenes_per_dataset']:.2f}\")\n    print(f\"  Average images per scene: {training_stats['avg_images_per_scene']:.2f}\")\n    print(f\"  Average outlier percentage: {training_stats['avg_outlier_percentage']:.2f}%\")\n    print(f\"  Datasets with outliers: {training_stats['outlier_datasets_percentage']:.2f}%\")\n    \n    # Check if thresholds are available\n    if training_csvs['thresholds'] is not None:\n        thresholds_df = training_csvs['thresholds']\n        print(\"\\nThreshold information available for scenes\")\nelse:\n    print(\"No training CSV files found. Using default parameters.\")\n    # Use default parameters\n    training_structure = {}\n    training_stats = {\n        'avg_scenes_per_dataset': 2.31,  # Based on previous analysis\n        'avg_images_per_scene': 60.77,\n        'avg_outlier_percentage': 5.85,\n        'outlier_datasets_percentage': 30.77\n    }\n\n# Set up test dataset info\ntest_dataset_info = {}\nfor dataset_path in Path(TEST_DIR).glob('*'):\n    if dataset_path.is_dir():\n        dataset_name = dataset_path.name\n        \n        # If we have this dataset in training, use its information\n        if dataset_name in training_structure:\n            expected_scene_count = training_structure[dataset_name]['expected_scene_count']\n            print(f\"Using scene count from training for {dataset_name}: {expected_scene_count} scenes\")\n            \n            test_dataset_info[dataset_name] = {\n                'path': dataset_path,\n                'expected_scene_count': expected_scene_count\n            }\n        else:\n            # No matching training data, use the average\n            avg_scenes = int(round(training_stats['avg_scenes_per_dataset']))\n            test_dataset_info[dataset_name] = {\n                'path': dataset_path,\n                'expected_scene_count': avg_scenes\n            }\n            print(f\"No training data for {dataset_name}, using average: {avg_scenes} scenes\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T05:53:48.190219Z","iopub.execute_input":"2025-04-04T05:53:48.190788Z","iopub.status.idle":"2025-04-04T05:53:48.274900Z","shell.execute_reply.started":"2025-04-04T05:53:48.190761Z","shell.execute_reply":"2025-04-04T05:53:48.274216Z"}},"outputs":[{"name":"stdout","text":"\nReading training CSV files...\nFound train_labels.csv with 1945 rows\nFound train_thresholds.csv with 30 rows\n\nExtracting ground truth information from training CSV files...\n\nTraining Structure from CSV:\n  ETs: 2 scenes, 22 images, 3 outliers (13.6%)\n  amy_gardens: 1 scenes, 200 images, 0 outliers (0.0%)\n  fbk_vineyard: 3 scenes, 163 images, 0 outliers (0.0%)\n  imc2023_haiper: 3 scenes, 54 images, 0 outliers (0.0%)\n  imc2023_heritage: 3 scenes, 209 images, 61 outliers (29.2%)\n  imc2023_theather_imc2024_church: 2 scenes, 76 images, 0 outliers (0.0%)\n  imc2024_dioscuri_baalshamin: 2 scenes, 138 images, 24 outliers (17.4%)\n  imc2024_lizard_pond: 2 scenes, 214 images, 34 outliers (15.9%)\n  pt_brandenburg_british_buckingham: 3 scenes, 225 images, 0 outliers (0.0%)\n  pt_piazzasanmarco_grandplace: 2 scenes, 168 images, 0 outliers (0.0%)\n  pt_sacrecoeur_trevi_tajmahal: 3 scenes, 225 images, 0 outliers (0.0%)\n  pt_stpeters_stpauls: 2 scenes, 200 images, 0 outliers (0.0%)\n  stairs: 2 scenes, 51 images, 0 outliers (0.0%)\n\nTraining Statistics from CSV:\n  Average scenes per dataset: 2.31\n  Average images per scene: 60.77\n  Average outlier percentage: 5.85%\n  Datasets with outliers: 30.77%\n\nThreshold information available for scenes\nUsing scene count from training for ETs: 2 scenes\nUsing scene count from training for stairs: 2 scenes\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## LightGlue Implementation","metadata":{}},{"cell_type":"code","source":"class LightGlueFeatureMatcher:\n    \"\"\"Feature extractor and matcher without relying on Kornia's LightGlue implementation.\"\"\"\n    \n    def __init__(self, max_keypoints=4000, use_disk=True):\n        \"\"\"Initialize custom feature extractor and matcher.\n        \n        Args:\n            max_keypoints: Maximum number of keypoints to detect\n            use_disk: Whether to use DISK (True) or SIFT (False) features\n        \"\"\"\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        self.max_keypoints = max_keypoints\n        self.use_disk = use_disk\n        \n        # Initialize as not available\n        self.available = False\n        self.extractor = None\n        \n        try:\n            # Import required modules\n            import kornia as K\n            import kornia.feature as KF\n            \n            # Initialize the appropriate extractor\n            if use_disk:\n                print(\"Initializing Custom DISK feature extractor...\")\n                # Check if we can use DISK\n                try:\n                    # Try to initialize DISK extractor\n                    self.extractor = KF.DISK.from_pretrained(\"depth\").to(self.device)\n                    print(\"DISK feature extractor initialized successfully\")\n                    self.available = True\n                except Exception as e:\n                    print(f\"Error initializing DISK extractor: {e}\")\n                    traceback.print_exc()\n            else:\n                print(\"Initializing Custom SIFT feature extractor...\")\n                # For SIFT, we don't need a pretrained extractor\n                self.available = True\n                print(\"SIFT feature extractor initialized successfully\")\n                \n        except Exception as e:\n            print(f\"Error initializing feature extractor: {e}\")\n            traceback.print_exc()\n    \n    def extract_disk(self, image_path):\n        \"\"\"Extract features using DISK with safer implementation.\n        \n        Args:\n            image_path: Path to the image\n            \n        Returns:\n            Dictionary with keypoints, descriptors, and dimensions\n        \"\"\"\n        if not self.available or not self.use_disk or self.extractor is None:\n            return None\n            \n        try:\n            import kornia as K\n            \n            # Load image\n            img = K.io.load_image(str(image_path), K.io.ImageLoadType.RGB32, device=self.device)[None, ...]\n            \n            # Extract features\n            with torch.no_grad():\n                # Try to extract features with the extractor\n                response = self.extractor(img, self.max_keypoints, pad_if_not_divisible=True)\n                \n                # Check if we have a proper response object\n                if hasattr(response, 'keypoints') and hasattr(response, 'descriptors'):\n                    # Convert to numpy\n                    keypoints = response.keypoints[0].cpu().numpy()\n                    descriptors = response.descriptors[0].cpu().numpy()\n                else:\n                    # Handle case where response is a tuple or different format\n                    print(\"DISK extractor response format unexpected, parsing manually...\")\n                    # Assuming it returns keypoints, descriptors as first two elements\n                    if isinstance(response, tuple) and len(response) >= 2:\n                        keypoints = response[0][0].cpu().numpy()\n                        descriptors = response[1][0].cpu().numpy()\n                    else:\n                        print(f\"Cannot parse DISK response: {type(response)}\")\n                        return None\n            \n            # Get image dimensions\n            dimensions = (img.shape[2], img.shape[3])  # (height, width)\n            \n            # Print feature statistics\n            print(f\"Extracted {len(keypoints)} DISK features from {image_path.name}\")\n            \n            return {\n                'keypoints': keypoints,\n                'descriptors': descriptors,\n                'dimensions': dimensions,\n                'path': image_path,\n                'backend': 'lightglue_disk'\n            }\n        except Exception as e:\n            print(f\"Error extracting DISK features: {e}\")\n            traceback.print_exc()\n            return None\n    \n    def extract_sift(self, image_path):\n        \"\"\"Extract features using OpenCV SIFT.\n        \n        Args:\n            image_path: Path to the image\n            \n        Returns:\n            Dictionary with keypoints, descriptors, and dimensions\n        \"\"\"\n        if not self.available or self.use_disk:\n            return None\n            \n        try:\n            # Load image with OpenCV for SIFT\n            img_cv = cv2.imread(str(image_path), cv2.IMREAD_GRAYSCALE)\n            if img_cv is None:\n                return None\n                \n            # Create SIFT detector with optimized parameters\n            sift = cv2.SIFT_create(\n                nfeatures=self.max_keypoints,\n                nOctaveLayers=5,\n                contrastThreshold=0.04,\n                edgeThreshold=15,\n                sigma=1.6\n            )\n            \n            # Detect keypoints and compute descriptors\n            cv_keypoints, cv_descriptors = sift.detectAndCompute(img_cv, None)\n            \n            if cv_keypoints is None or len(cv_keypoints) == 0:\n                return None\n            \n            # Convert keypoints to numpy arrays\n            keypoints = np.array([kp.pt for kp in cv_keypoints])\n            \n            # Convert descriptors to proper format\n            descriptors = cv_descriptors.astype(np.float32)\n            \n            # Get image dimensions\n            dimensions = img_cv.shape\n            \n            # Print feature statistics\n            print(f\"Extracted {len(keypoints)} SIFT features from {image_path.name}\")\n            \n            return {\n                'keypoints': keypoints,\n                'descriptors': descriptors,\n                'dimensions': dimensions,\n                'path': image_path,\n                'backend': 'lightglue_sift'\n            }\n        except Exception as e:\n            print(f\"Error extracting SIFT features: {e}\")\n            traceback.print_exc()\n            return None\n    \n    def extract(self, image_path):\n        \"\"\"Extract features based on the chosen method (DISK or SIFT).\n        \n        Args:\n            image_path: Path to the image\n            \n        Returns:\n            Dictionary with keypoints, descriptors, and dimensions\n        \"\"\"\n        if self.use_disk:\n            return self.extract_disk(image_path)\n        else:\n            return self.extract_sift(image_path)\n    \n    def match_disk(self, features1, features2):\n        \"\"\"Match DISK features between two images using custom matcher.\n        \n        Args:\n            features1: Features from the first image\n            features2: Features from the second image\n            \n        Returns:\n            List of matches as cv2.DMatch objects\n        \"\"\"\n        if not self.available or not self.use_disk:\n            return []\n        \n        # Use custom matcher based on cosine similarity\n        return self._match_descriptors(features1['descriptors'], features2['descriptors'])\n    \n    def match_sift(self, features1, features2):\n        \"\"\"Match SIFT features between two images using custom matcher.\n        \n        Args:\n            features1: Features from the first image\n            features2: Features from the second image\n            \n        Returns:\n            List of matches as cv2.DMatch objects\n        \"\"\"\n        if not self.available or self.use_disk:\n            return []\n        \n        # Use custom matcher based on cosine similarity\n        return self._match_descriptors(features1['descriptors'], features2['descriptors'])\n    \n    def _match_descriptors(self, desc1, desc2):\n        \"\"\"Match descriptors using cosine similarity and ratio test.\n        \n        Args:\n            desc1, desc2: Feature descriptors as numpy arrays\n            \n        Returns:\n            List of cv2.DMatch objects\n        \"\"\"\n        try:\n            # Fix descriptor orientation if necessary\n            # SIFT descriptors are typically (N, 128) while SuperPoint are (256, N)\n            if desc1.shape[0] < desc1.shape[1]:\n                # Assume descriptors are in the format (N, D)\n                pass\n            else:\n                # Transpose descriptors from (D, N) to (N, D)\n                desc1 = desc1.T\n                desc2 = desc2.T\n            \n            # Print descriptor shapes for debugging\n            print(f\"Matching descriptors with shapes: {desc1.shape} and {desc2.shape}\")\n            \n            # Normalize descriptors for cosine similarity\n            desc1_norm = desc1 / (np.linalg.norm(desc1, axis=1, keepdims=True) + 1e-8)\n            desc2_norm = desc2 / (np.linalg.norm(desc2, axis=1, keepdims=True) + 1e-8)\n            \n            # Compute similarity matrix\n            similarity = desc1_norm @ desc2_norm.T\n            \n            # Apply ratio test\n            good_matches = []\n            for i in range(similarity.shape[0]):\n                # Get similarities for this descriptor\n                scores = similarity[i]\n                \n                # Find best match\n                best_idx = np.argmax(scores)\n                best_score = scores[best_idx]\n                \n                # Find second best match\n                scores_copy = scores.copy()\n                scores_copy[best_idx] = -1\n                second_best_idx = np.argmax(scores_copy)\n                second_best_score = scores_copy[second_best_idx]\n                \n                # Apply ratio test (lower ratio means stricter test)\n                ratio = 0.8\n                if best_score > 0.6 and best_score > ratio * second_best_score:\n                    m = cv2.DMatch()\n                    m.queryIdx = i\n                    m.trainIdx = best_idx\n                    m.distance = 1.0 - best_score  # Convert similarity to distance\n                    good_matches.append(m)\n            \n            print(f\"Found {len(good_matches)} matches with ratio test\")\n            return good_matches\n        except Exception as e:\n            print(f\"Error in matching descriptors: {e}\")\n            traceback.print_exc()\n            return []\n    \n    def match(self, features1, features2):\n        \"\"\"Match features based on the chosen method (DISK or SIFT).\n        \n        Args:\n            features1: Features from the first image\n            features2: Features from the second image\n            \n        Returns:\n            List of matches as cv2.DMatch objects\n        \"\"\"\n        if self.use_disk:\n            return self.match_disk(features1, features2)\n        else:\n            return self.match_sift(features1, features2)\n\n# Test LightGlue initialization\nprint(\"\\nInitializing custom feature extractors...\")\ndisk_extractor = LightGlueFeatureMatcher(use_disk=True)\nsift_extractor = LightGlueFeatureMatcher(use_disk=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T06:41:15.495082Z","iopub.execute_input":"2025-04-04T06:41:15.495434Z","iopub.status.idle":"2025-04-04T06:41:15.535256Z","shell.execute_reply.started":"2025-04-04T06:41:15.495405Z","shell.execute_reply":"2025-04-04T06:41:15.534451Z"}},"outputs":[{"name":"stdout","text":"\nInitializing custom feature extractors...\nInitializing Custom DISK feature extractor...\nDISK feature extractor initialized successfully\nInitializing Custom SIFT feature extractor...\nSIFT feature extractor initialized successfully\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"## SuperPoint and SuperGlue Implementation","metadata":{}},{"cell_type":"code","source":"# Create weights directory if it doesn't exist\nos.makedirs(f\"{SUPERGLUE_DIR}/weights\", exist_ok=True)\n\n# Download SuperPoint and SuperGlue models to base directory\nif not os.path.exists(f\"{SUPERGLUE_DIR}/superpoint_v1.pth\"):\n    print(\"Downloading SuperPoint model...\")\n    !wget -q -O {SUPERGLUE_DIR}/superpoint_v1.pth https://github.com/magicleap/SuperGluePretrainedNetwork/raw/master/models/weights/superpoint_v1.pth\n\nif not os.path.exists(f\"{SUPERGLUE_DIR}/superglue_outdoor.pth\"):\n    print(\"Downloading SuperGlue outdoor model...\")\n    !wget -q -O {SUPERGLUE_DIR}/superglue_outdoor.pth https://github.com/magicleap/SuperGluePretrainedNetwork/raw/master/models/weights/superglue_outdoor.pth\n\n# Now copy the files to weights directory with the correct names\n# For SuperPoint, copy to both the original name and 'superpoint_v1.pth'\nif not os.path.exists(f\"{SUPERGLUE_DIR}/weights/superpoint_v1.pth\"):\n    print(\"Copying SuperPoint model to weights directory...\")\n    !cp {SUPERGLUE_DIR}/superpoint_v1.pth {SUPERGLUE_DIR}/weights/\n\n# For SuperGlue, we need to rename to just 'outdoor.pth'\nif not os.path.exists(f\"{SUPERGLUE_DIR}/weights/outdoor.pth\"):\n    print(\"Copying SuperGlue model to weights directory with correct name...\")\n    !cp {SUPERGLUE_DIR}/superglue_outdoor.pth {SUPERGLUE_DIR}/weights/outdoor.pth\n\n# Verify files exist in the expected locations\nprint(f\"SuperPoint model in base dir: {os.path.exists(f'{SUPERGLUE_DIR}/superpoint_v1.pth')}\")\nprint(f\"SuperPoint model in weights dir: {os.path.exists(f'{SUPERGLUE_DIR}/weights/superpoint_v1.pth')}\")\nprint(f\"SuperGlue model in base dir: {os.path.exists(f'{SUPERGLUE_DIR}/superglue_outdoor.pth')}\")\nprint(f\"SuperGlue model in weights dir (renamed): {os.path.exists(f'{SUPERGLUE_DIR}/weights/outdoor.pth')}\")\n\n# Download Python modules if needed\nif not os.path.exists(f\"{SUPERGLUE_DIR}/superpoint.py\"):\n    print(\"Downloading SuperPoint Python module...\")\n    !wget -q -O {SUPERGLUE_DIR}/superpoint.py https://raw.githubusercontent.com/magicleap/SuperGluePretrainedNetwork/master/models/superpoint.py\n\nif not os.path.exists(f\"{SUPERGLUE_DIR}/superglue.py\"):\n    print(\"Downloading SuperGlue Python module...\")\n    !wget -q -O {SUPERGLUE_DIR}/superglue.py https://raw.githubusercontent.com/magicleap/SuperGluePretrainedNetwork/master/models/superglue.py\n\nif not os.path.exists(f\"{SUPERGLUE_DIR}/__init__.py\"):\n    print(\"Downloading __init__.py module...\")\n    !wget -q -O {SUPERGLUE_DIR}/__init__.py https://raw.githubusercontent.com/magicleap/SuperGluePretrainedNetwork/master/models/__init__.py\n\n# Fix the SuperGlue source code to avoid the 'shape' attribute error\ndef fix_superglue_source():\n    \"\"\"Fix the SuperGlue source code to handle the shape attribute properly.\"\"\"\n    superglue_path = f\"{SUPERGLUE_DIR}/superglue.py\"\n    if not os.path.exists(superglue_path):\n        print(\"SuperGlue source not found\")\n        return\n    \n    # Read the file\n    with open(superglue_path, 'r') as f:\n        content = f.read()\n    \n    # Fix the normalize_keypoints function call\n    if \"kpts0 = normalize_keypoints(kpts0, data['image0'].shape)\" in content:\n        modified_content = content.replace(\n            \"kpts0 = normalize_keypoints(kpts0, data['image0'].shape)\",\n            \"kpts0 = normalize_keypoints(kpts0, data['image0']['shape'])\"\n        )\n        modified_content = modified_content.replace(\n            \"kpts1 = normalize_keypoints(kpts1, data['image1'].shape)\",\n            \"kpts1 = normalize_keypoints(kpts1, data['image1']['shape'])\"\n        )\n        \n        # Write the modified content\n        with open(superglue_path, 'w') as f:\n            f.write(modified_content)\n        print(\"Fixed SuperGlue source code to handle image shape properly\")\n    else:\n        print(\"Could not find the line to fix in SuperGlue source\")\n\n# Fix the SuperGlue source\nfix_superglue_source()\n\nclass SimpleKeypointDetector:\n    \"\"\"Base class for keypoint detectors.\"\"\"\n    \n    def __init__(self):\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nclass SimpleSuperPointDetector(SimpleKeypointDetector):\n    \"\"\"SuperPoint feature detector and descriptor.\"\"\"\n    \n    def __init__(self, max_keypoints=4000, keypoint_threshold=0.005):\n        \"\"\"Initialize SuperPoint model.\n        \n        Args:\n            max_keypoints: Maximum number of keypoints to detect\n            keypoint_threshold: Keypoint confidence threshold\n        \"\"\"\n        super().__init__()\n        \n        # Check if SuperGlue is available\n        if not os.path.exists(f\"{SUPERGLUE_DIR}/superpoint.py\"):\n            print(\"SuperPoint not available - required files missing\")\n            self.detector = None\n            return\n            \n        try:\n            # Import the SuperPoint model\n            sys.path.append(SUPERGLUE_DIR)\n            from superpoint import SuperPoint\n            \n            # Configure SuperPoint\n            config = {\n                'nms_radius': 4,\n                'keypoint_threshold': keypoint_threshold,\n                'max_keypoints': max_keypoints,\n                # SuperPoint looks for 'superpoint_v1.pth' by default\n                'weights': 'superpoint_v1'\n            }\n            \n            # Create the model\n            self.detector = SuperPoint(config)\n            \n            # Load weights manually from one of the possible paths\n            if os.path.exists(f\"{SUPERGLUE_DIR}/weights/superpoint_v1.pth\"):\n                weights_path = f\"{SUPERGLUE_DIR}/weights/superpoint_v1.pth\"\n            elif os.path.exists(f\"{SUPERGLUE_DIR}/superpoint_v1.pth\"):\n                weights_path = f\"{SUPERGLUE_DIR}/superpoint_v1.pth\"\n            else:\n                raise FileNotFoundError(\"SuperPoint weights not found\")\n                \n            print(f\"Loading SuperPoint model from {weights_path}\")\n            state_dict = torch.load(weights_path, map_location=self.device)\n            self.detector.load_state_dict(state_dict)\n            \n            # Move to device and set to eval mode\n            self.detector = self.detector.to(self.device).eval()\n            print(\"SuperPoint model loaded successfully\")\n        except Exception as e:\n            print(f\"Error loading SuperPoint model: {e}\")\n            traceback.print_exc()\n            self.detector = None\n    \n    def detect_and_compute(self, image):\n        \"\"\"Detect keypoints and compute descriptors.\n        \n        Args:\n            image: Input image in BGR format\n            \n        Returns:\n            Tuple of (keypoints, descriptors, scores) or None if detection fails\n        \"\"\"\n        if self.detector is None:\n            return None\n            \n        try:\n            # Convert image to grayscale if necessary\n            if len(image.shape) == 3:\n                gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n            else:\n                gray = image\n            \n            # Normalize and convert to tensor\n            img = torch.from_numpy(gray).float() / 255.0\n            img = img.unsqueeze(0).unsqueeze(0).to(self.device)\n            \n            # Detect keypoints and compute descriptors\n            with torch.no_grad():\n                pred = self.detector({'image': img})\n            \n            # Convert results to numpy arrays\n            keypoints = pred['keypoints'][0].cpu().numpy()\n            scores = pred['scores'][0].cpu().numpy()\n            descriptors = pred['descriptors'][0].cpu().numpy()\n            \n            # Print shape information for debugging\n            print(f\"SuperPoint: Found {len(keypoints)} keypoints\")\n            print(f\"Keypoints shape: {keypoints.shape}, Descriptors shape: {descriptors.shape}\")\n            \n            return keypoints, descriptors, scores\n        except Exception as e:\n            print(f\"Error detecting keypoints: {e}\")\n            traceback.print_exc()\n            return None\n\nclass SimpleSuperGlueMatcher:\n    \"\"\"SuperGlue feature matcher.\"\"\"\n    \n    def __init__(self, match_threshold=0.2):\n        \"\"\"Initialize SuperGlue model.\n        \n        Args:\n            match_threshold: Matching confidence threshold\n        \"\"\"\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        self.match_threshold = match_threshold\n        \n        # Check if SuperGlue is available\n        if not os.path.exists(f\"{SUPERGLUE_DIR}/superglue.py\"):\n            print(\"SuperGlue not available - required files missing\")\n            self.matcher = None\n            return\n            \n        try:\n            # Import the SuperGlue model\n            sys.path.append(SUPERGLUE_DIR)\n            from superglue import SuperGlue\n            \n            # Configure SuperGlue\n            config = {\n                'weights': 'outdoor',  # Will look for weights/outdoor.pth\n                'sinkhorn_iterations': 20,\n                'match_threshold': match_threshold\n            }\n            \n            # Create the model\n            self.matcher = SuperGlue(config)\n            \n            # Load weights manually from one of the possible paths\n            if os.path.exists(f\"{SUPERGLUE_DIR}/weights/outdoor.pth\"):\n                weights_path = f\"{SUPERGLUE_DIR}/weights/outdoor.pth\"\n            elif os.path.exists(f\"{SUPERGLUE_DIR}/superglue_outdoor.pth\"):\n                weights_path = f\"{SUPERGLUE_DIR}/superglue_outdoor.pth\"\n            else:\n                raise FileNotFoundError(\"SuperGlue weights not found\")\n                \n            print(f\"Loading SuperGlue model from {weights_path}\")\n            state_dict = torch.load(weights_path, map_location=self.device)\n            self.matcher.load_state_dict(state_dict)\n            \n            # Move to device and set to eval mode\n            self.matcher = self.matcher.to(self.device).eval()\n            print(\"SuperGlue model loaded successfully\")\n        except Exception as e:\n            print(f\"Error loading SuperGlue model: {e}\")\n            traceback.print_exc()\n            self.matcher = None\n    \n    def match(self, kp1, desc1, kp2, desc2, scores1=None, scores2=None):\n        \"\"\"Match features between two images.\n        \n        Args:\n            kp1, kp2: Keypoints from the first and second images\n            desc1, desc2: Descriptors from the first and second images\n            scores1, scores2: Keypoint scores (optional)\n            \n        Returns:\n            List of cv2.DMatch objects\n        \"\"\"\n        if self.matcher is None:\n            # Fall back to traditional matching using custom matcher\n            return self._match_custom(desc1, desc2)\n            \n        try:\n            # Print shape info for debugging\n            print(f\"SuperGlue: kp1 shape: {kp1.shape}, desc1 shape: {desc1.shape}\")\n            print(f\"SuperGlue: kp2 shape: {kp2.shape}, desc2 shape: {desc2.shape}\")\n            \n            # Fix descriptor orientation if necessary (SuperPoint outputs descriptors as D×N)\n            if desc1.shape[0] == 256 and desc1.shape[1] != 256:\n                # Already in the correct format (D×N)\n                pass\n            elif desc1.shape[1] == 256 and desc1.shape[0] != 256:\n                # Need to transpose from N×D to D×N\n                desc1 = desc1.T\n                desc2 = desc2.T\n                print(f\"Transposed descriptors to: {desc1.shape} and {desc2.shape}\")\n            \n            # Prepare data for SuperGlue\n            if scores1 is None:\n                scores1 = np.ones(len(kp1))\n            if scores2 is None:\n                scores2 = np.ones(len(kp2))\n                \n            # Convert to torch tensors\n            data = {\n                'keypoints0': torch.from_numpy(kp1).float().to(self.device)[None],\n                'keypoints1': torch.from_numpy(kp2).float().to(self.device)[None],\n                'descriptors0': torch.from_numpy(desc1).float().to(self.device)[None],\n                'descriptors1': torch.from_numpy(desc2).float().to(self.device)[None],\n                'scores0': torch.from_numpy(scores1).float().to(self.device)[None],\n                'scores1': torch.from_numpy(scores2).float().to(self.device)[None],\n                # Add image shapes for SuperGlue (as dictionary)\n                'image0': {'shape': torch.tensor([1, 1, kp1.shape[0], desc1.shape[1]], device=self.device)},\n                'image1': {'shape': torch.tensor([1, 1, kp2.shape[0], desc2.shape[1]], device=self.device)}\n            }\n            \n            # Match features\n            with torch.no_grad():\n                pred = self.matcher(data)\n            \n            # Convert results to list of DMatch objects\n            matches = pred['matches0'][0].cpu().numpy()\n            confidences = pred['matching_scores0'][0].cpu().numpy()\n            \n            # Create list of DMatch objects\n            good_matches = []\n            for i, idx in enumerate(matches):\n                if idx >= 0 and confidences[i] > self.match_threshold:\n                    m = cv2.DMatch()\n                    m.queryIdx = i\n                    m.trainIdx = idx\n                    m.distance = 1.0 - confidences[i]  # Convert confidence to distance\n                    good_matches.append(m)\n            \n            print(f\"SuperGlue: Found {len(good_matches)} matches\")\n            return good_matches\n        except Exception as e:\n            print(f\"Error matching features: {e}\")\n            traceback.print_exc()\n            # Fall back to traditional matching\n            return self._match_custom(desc1, desc2)\n    \n    def _match_custom(self, desc1, desc2):\n        \"\"\"Match descriptors using custom similarity-based matcher.\n        \n        Args:\n            desc1, desc2: Feature descriptors\n            \n        Returns:\n            List of cv2.DMatch objects\n        \"\"\"\n        try:\n            # Fix descriptor orientation if necessary (SuperPoint outputs descriptors as D×N)\n            if desc1.shape[0] == 256 and desc1.shape[1] != 256:\n                # Transpose from D×N to N×D\n                desc1 = desc1.T\n                desc2 = desc2.T\n                print(f\"Transposed descriptors for matching: {desc1.shape} and {desc2.shape}\")\n            \n            # Normalize descriptors for cosine similarity\n            desc1_norm = np.copy(desc1)\n            desc2_norm = np.copy(desc2)\n            \n            # Normalize rows\n            desc1_norms = np.linalg.norm(desc1_norm, axis=1, keepdims=True)\n            desc2_norms = np.linalg.norm(desc2_norm, axis=1, keepdims=True)\n            \n            # Avoid division by zero\n            desc1_norm = np.divide(desc1_norm, desc1_norms, out=np.zeros_like(desc1_norm), where=desc1_norms!=0)\n            desc2_norm = np.divide(desc2_norm, desc2_norms, out=np.zeros_like(desc2_norm), where=desc2_norms!=0)\n            \n            # Compute similarity matrix\n            similarity = desc1_norm @ desc2_norm.T\n            \n            # Apply ratio test\n            good_matches = []\n            for i in range(similarity.shape[0]):\n                # Get similarities for this descriptor\n                scores = similarity[i]\n                \n                # Find best match\n                best_idx = np.argmax(scores)\n                best_score = scores[best_idx]\n                \n                # Find second best match\n                scores_copy = scores.copy()\n                scores_copy[best_idx] = -1\n                second_best_idx = np.argmax(scores_copy)\n                second_best_score = scores_copy[second_best_idx]\n                \n                # Apply ratio test (lower ratio means stricter test)\n                ratio = 0.8\n                if best_score > 0.6 and best_score > ratio * second_best_score:\n                    m = cv2.DMatch()\n                    m.queryIdx = i\n                    m.trainIdx = best_idx\n                    m.distance = 1.0 - best_score  # Convert similarity to distance\n                    good_matches.append(m)\n            \n            print(f\"Custom matcher: Found {len(good_matches)} matches\")\n            return good_matches\n        except Exception as e:\n            print(f\"Error in custom matching: {e}\")\n            traceback.print_exc()\n            return []\n\n# Initialize SuperPoint and SuperGlue\nprint(\"\\nInitializing SuperPoint and SuperGlue...\")\nsuperpoint_detector = SimpleSuperPointDetector()\nsuperglue_matcher = SimpleSuperGlueMatcher()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T06:42:19.378445Z","iopub.execute_input":"2025-04-04T06:42:19.378789Z","iopub.status.idle":"2025-04-04T06:42:19.836196Z","shell.execute_reply.started":"2025-04-04T06:42:19.378765Z","shell.execute_reply":"2025-04-04T06:42:19.835302Z"}},"outputs":[{"name":"stdout","text":"SuperPoint model in base dir: True\nSuperPoint model in weights dir: True\nSuperGlue model in base dir: True\nSuperGlue model in weights dir (renamed): True\nFixed SuperGlue source code to handle image shape properly\n\nInitializing SuperPoint and SuperGlue...\nLoaded SuperPoint model\nLoading SuperPoint model from /kaggle/working/superglue_models/weights/superpoint_v1.pth\nSuperPoint model loaded successfully\nLoaded SuperGlue model (\"outdoor\" weights)\nLoading SuperGlue model from /kaggle/working/superglue_models/weights/outdoor.pth\nSuperGlue model loaded successfully\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"## Multi-Backend Feature Extraction and Matching System","metadata":{}},{"cell_type":"code","source":"class MultiBackendMatcher:\n    \"\"\"Feature extractor and matcher with multiple backend options.\"\"\"\n    \n    def __init__(self, feature_params, matching_params):\n        \"\"\"Initialize with multiple feature extraction and matching backends.\n        \n        Args:\n            feature_params: Parameters for feature detection\n            matching_params: Parameters for feature matching\n        \"\"\"\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        self.feature_params = feature_params\n        self.matching_params = matching_params\n        \n        # Available backends list\n        self.backends = []\n        \n        # Initialize LightGlue with DISK if available\n        try:\n            self.lightglue_disk = LightGlueFeatureMatcher(use_disk=True)\n            if self.lightglue_disk.available:\n                print(\"LightGlue with DISK features is available\")\n                self.backends.append(\"lightglue_disk\")\n        except Exception as e:\n            print(f\"LightGlue with DISK features is not available: {e}\")\n        \n        # Initialize LightGlue with SIFT if available\n        try:\n            self.lightglue_sift = LightGlueFeatureMatcher(use_disk=False)\n            if self.lightglue_sift.available:\n                print(\"LightGlue with SIFT features is available\")\n                self.backends.append(\"lightglue_sift\")\n        except Exception as e:\n            print(f\"LightGlue with SIFT features is not available: {e}\")\n        \n        # Initialize SuperPoint/SuperGlue if available\n        try:\n            self.superpoint = SimpleSuperPointDetector()\n            self.superglue = SimpleSuperGlueMatcher()\n            \n            if self.superpoint.detector is not None and self.superglue.matcher is not None:\n                print(\"SuperPoint/SuperGlue is available\")\n                self.backends.append(\"superglue\")\n        except Exception as e:\n            print(f\"SuperPoint/SuperGlue is not available: {e}\")\n        \n        # Always add traditional SIFT as a fallback\n        self.backends.append(\"sift\")\n        print(\"Traditional SIFT is available as fallback\")\n        \n        # Report available backends\n        if self.backends:\n            primary_backend = self.backends[0]\n            print(f\"Using {primary_backend} as primary backend\")\n            print(f\"Available backends (in order): {', '.join(self.backends)}\")\n        else:\n            print(\"Warning: No feature extraction backends available\")\n    \n    def extract(self, image_path):\n        \"\"\"Extract features from an image using the best available backend.\n        \n        Args:\n            image_path: Path to the image\n            \n        Returns:\n            Dictionary with keypoints, descriptors, and dimensions, or None if extraction fails\n        \"\"\"\n        features = None\n        backend_used = None\n        \n        # Try each backend in order until one succeeds\n        for backend in self.backends:\n            try:\n                if backend == \"lightglue_disk\":\n                    features = self.lightglue_disk.extract(image_path)\n                elif backend == \"lightglue_sift\":\n                    features = self.lightglue_sift.extract(image_path)\n                elif backend == \"superglue\":\n                    features = self._extract_superpoint(image_path)\n                elif backend == \"sift\":\n                    features = self._extract_sift(image_path)\n                \n                if features is not None and len(features.get('keypoints', [])) > 0:\n                    backend_used = backend\n                    if 'backend' not in features:\n                        features['backend'] = backend\n                    break\n            except Exception as e:\n                print(f\"Error extracting features with {backend}: {e}\")\n                traceback.print_exc()\n        \n        if features is None:\n            print(f\"All feature extraction backends failed for {image_path}\")\n        else:\n            print(f\"Extracted features from {image_path.name} using {backend_used} backend\")\n        \n        return features\n    \n    def _extract_superpoint(self, image_path):\n        \"\"\"Extract features using SuperPoint.\n        \n        Args:\n            image_path: Path to the image\n            \n        Returns:\n            Dictionary with features or None if extraction fails\n        \"\"\"\n        if self.superpoint.detector is None:\n            return None\n        \n        try:\n            # Load image\n            img = cv2.imread(str(image_path))\n            if img is None:\n                print(f\"Could not read image: {image_path}\")\n                return None\n            \n            # Extract features\n            result = self.superpoint.detect_and_compute(img)\n            if result is None:\n                return None\n            \n            keypoints, descriptors, scores = result\n            \n            return {\n                'keypoints': keypoints,\n                'descriptors': descriptors,\n                'scores': scores,\n                'dimensions': img.shape[:2] if len(img.shape) == 3 else img.shape,\n                'path': image_path,\n                'backend': 'superglue'\n            }\n        except Exception as e:\n            print(f\"Error extracting SuperPoint features: {e}\")\n            traceback.print_exc()\n            return None\n    \n    def _extract_sift(self, image_path):\n        \"\"\"Extract features using SIFT.\n        \n        Args:\n            image_path: Path to the image\n            \n        Returns:\n            Dictionary with features or None if extraction fails\n        \"\"\"\n        try:\n            # Load image\n            img = cv2.imread(str(image_path), cv2.IMREAD_GRAYSCALE)\n            if img is None:\n                print(f\"Could not read image: {image_path}\")\n                return None\n                \n            # Create SIFT detector with optimized parameters\n            sift = cv2.SIFT_create(\n                nfeatures=4000,\n                nOctaveLayers=5,\n                contrastThreshold=0.04,\n                edgeThreshold=15,\n                sigma=1.6\n            )\n            \n            # Detect keypoints and compute descriptors\n            keypoints, descriptors = sift.detectAndCompute(img, None)\n            \n            if keypoints is None or len(keypoints) == 0:\n                return None\n                \n            print(f\"Extracted {len(keypoints)} SIFT features from {image_path.name}\")\n            \n            return {\n                'keypoints': keypoints,\n                'descriptors': descriptors,\n                'dimensions': img.shape,\n                'path': image_path,\n                'backend': 'sift'\n            }\n        except Exception as e:\n            print(f\"Error extracting SIFT features: {e}\")\n            traceback.print_exc()\n            return None\n    \n    def match(self, features1, features2):\n        \"\"\"Match features between two images based on their backends.\n        \n        Args:\n            features1: Features from the first image\n            features2: Features from the second image\n            \n        Returns:\n            Tuple of (matches, inlier_count, is_geometrically_consistent)\n        \"\"\"\n        # Get backend types\n        backend1 = features1.get('backend', 'unknown')\n        backend2 = features2.get('backend', 'unknown')\n        \n        # Check if both features use the same backend\n        if backend1 != backend2:\n            print(f\"Warning: Mixing feature backends ({backend1} and {backend2})\")\n        \n        # Attempt to match based on backend type\n        matches = None\n        try:\n            if backend1.startswith('lightglue_disk') and backend2.startswith('lightglue_disk'):\n                matches = self.lightglue_disk.match(features1, features2)\n            elif backend1.startswith('lightglue_sift') and backend2.startswith('lightglue_sift'):\n                matches = self.lightglue_sift.match(features1, features2)\n            elif backend1 == 'superglue' and backend2 == 'superglue':\n                matches = self.superglue.match(\n                    features1['keypoints'], features1['descriptors'],\n                    features2['keypoints'], features2['descriptors'],\n                    features1.get('scores'), features2.get('scores')\n                )\n            else:\n                # Fall back to appropriate matching based on keypoint type\n                if hasattr(features1['keypoints'][0], 'pt'):  # CV2 KeyPoint objects (SIFT)\n                    matches = self._match_cv2_keypoints(features1, features2)\n                else:  # Raw keypoints array\n                    matches = self._match_raw_keypoints(features1, features2)\n        except Exception as e:\n            print(f\"Error in feature matching: {e}\")\n            traceback.print_exc()\n            # Try generic descriptor matching as last resort\n            try:\n                matches = self._match_descriptors(\n                    features1['descriptors'], \n                    features2['descriptors'],\n                    features1.get('backend'), \n                    features2.get('backend')\n                )\n            except Exception as e2:\n                print(f\"Generic descriptor matching also failed: {e2}\")\n                traceback.print_exc()\n                return [], 0, False\n        \n        # Verify matches geometrically if we have matches\n        if matches and len(matches) > 0:\n            return self._verify_matches(matches, features1, features2)\n        else:\n            return [], 0, False\n    \n    def _match_cv2_keypoints(self, features1, features2):\n        \"\"\"Match OpenCV KeyPoint features using FLANN.\n        \n        Args:\n            features1, features2: Feature dictionaries with cv2.KeyPoint objects\n            \n        Returns:\n            List of cv2.DMatch objects\n        \"\"\"\n        # Use custom feature matching instead of FLANN to avoid dimensionality issues\n        return self._match_descriptors(\n            features1['descriptors'], \n            features2['descriptors'],\n            features1.get('backend'), \n            features2.get('backend')\n        )\n    \n    def _match_raw_keypoints(self, features1, features2):\n        \"\"\"Match raw keypoint features (SuperPoint or LightGlue).\n        \n        Args:\n            features1, features2: Feature dictionaries with raw keypoint arrays\n            \n        Returns:\n            List of cv2.DMatch objects\n        \"\"\"\n        return self._match_descriptors(\n            features1['descriptors'], \n            features2['descriptors'],\n            features1.get('backend'), \n            features2.get('backend')\n        )\n    \n    def _match_descriptors(self, desc1, desc2, backend1=None, backend2=None):\n        \"\"\"Match descriptors using cosine similarity and ratio test.\n        This is a safer generic method that works with any descriptor format.\n        \n        Args:\n            desc1, desc2: Feature descriptors as numpy arrays\n            backend1, backend2: Optional backend names for debugging\n            \n        Returns:\n            List of cv2.DMatch objects\n        \"\"\"\n        try:\n            # Print original descriptor shapes\n            print(f\"Original descriptor shapes - desc1: {desc1.shape}, desc2: {desc2.shape}\")\n            \n            # Fix descriptor orientation if necessary\n            if backend1 == 'superglue' or backend2 == 'superglue':\n                # SuperPoint descriptors are typically (256, N)\n                if desc1.shape[0] == 256 and desc1.shape[1] != 256:\n                    # Transpose from (256, N) to (N, 256)\n                    desc1 = desc1.T\n                if desc2.shape[0] == 256 and desc2.shape[1] != 256:\n                    # Transpose from (256, N) to (N, 256)\n                    desc2 = desc2.T\n                print(f\"Transposed SuperPoint descriptors to: {desc1.shape} and {desc2.shape}\")\n            elif hasattr(desc1, 'shape') and len(desc1.shape) == 1:\n                # Handle 1D descriptors (reshape to 2D)\n                desc1 = desc1.reshape(1, -1)\n                desc2 = desc2.reshape(1, -1)\n                print(f\"Reshaped 1D descriptors to: {desc1.shape} and {desc2.shape}\")\n            elif isinstance(desc1, list):\n                # Handle list descriptors\n                desc1 = np.array(desc1)\n                desc2 = np.array(desc2)\n                print(f\"Converted list descriptors to arrays: {desc1.shape} and {desc2.shape}\")\n            \n            # Ensure we have 2D arrays with features as rows\n            if len(desc1.shape) != 2 or len(desc2.shape) != 2:\n                print(f\"Unexpected descriptor shapes: {desc1.shape} and {desc2.shape}\")\n                # Try to reshape if possible\n                if len(desc1.shape) > 2:\n                    desc1 = desc1.reshape(desc1.shape[0], -1)\n                if len(desc2.shape) > 2:\n                    desc2 = desc2.reshape(desc2.shape[0], -1)\n            \n            # Ensure features are rows (N, D) not columns (D, N)\n            if desc1.shape[0] < desc1.shape[1]:\n                # Already (N, D) format\n                pass\n            else:\n                # Try to determine if we need to transpose\n                # For SIFT: (N, 128), for SuperPoint: (256, N)\n                is_superpoint = (desc1.shape[0] == 256 or desc1.shape[1] == 256)\n                \n                if is_superpoint:\n                    # SuperPoint descriptors - transpose if needed\n                    if desc1.shape[0] != desc1.shape[1] and desc1.shape[0] > desc1.shape[1]:\n                        # More rows than columns - probably needs transpose\n                        desc1 = desc1.T\n                    if desc2.shape[0] != desc2.shape[1] and desc2.shape[0] > desc2.shape[1]:\n                        desc2 = desc2.T\n            \n            # Print final descriptor shapes\n            print(f\"Final descriptor shapes for matching - desc1: {desc1.shape}, desc2: {desc2.shape}\")\n            \n            # Normalize descriptors for cosine similarity\n            desc1_norm = np.copy(desc1)\n            desc2_norm = np.copy(desc2)\n            \n            # Normalize rows\n            desc1_norms = np.linalg.norm(desc1_norm, axis=1, keepdims=True) + 1e-10\n            desc2_norms = np.linalg.norm(desc2_norm, axis=1, keepdims=True) + 1e-10\n            \n            desc1_norm = desc1_norm / desc1_norms\n            desc2_norm = desc2_norm / desc2_norms\n            \n            # Compute similarity matrix\n            similarity = desc1_norm @ desc2_norm.T\n            \n            # Apply ratio test\n            good_matches = []\n            for i in range(similarity.shape[0]):\n                # Get similarities for this descriptor\n                scores = similarity[i]\n                \n                # Find best match\n                best_idx = np.argmax(scores)\n                best_score = scores[best_idx]\n                \n                # Find second best match\n                scores_copy = scores.copy()\n                scores_copy[best_idx] = -1\n                second_best_idx = np.argmax(scores_copy)\n                second_best_score = scores_copy[second_best_idx]\n                \n                # Apply ratio test (lower ratio means stricter test)\n                ratio = 0.8\n                if best_score > 0.6 and best_score > ratio * second_best_score:\n                    m = cv2.DMatch()\n                    m.queryIdx = i\n                    m.trainIdx = best_idx\n                    m.distance = 1.0 - best_score  # Convert similarity to distance\n                    good_matches.append(m)\n            \n            print(f\"Generic descriptor matcher: Found {len(good_matches)} matches\")\n            return good_matches\n        except Exception as e:\n            print(f\"Error in generic descriptor matching: {e}\")\n            traceback.print_exc()\n            return []\n    \n    def _verify_matches(self, matches, features1, features2):\n        \"\"\"Verify matches using geometric constraints.\n        \n        Args:\n            matches: List of cv2.DMatch objects\n            features1, features2: Feature dictionaries\n            \n        Returns:\n            Tuple of (filtered_matches, inlier_count, is_geometrically_consistent)\n        \"\"\"\n        # Check if we have enough matches\n        if len(matches) < 8:\n            return [], 0, False\n        \n        try:\n            # Extract matched points\n            # Handle different keypoint types (cv2.KeyPoint vs. raw arrays)\n            is_keypoint_object = hasattr(features1['keypoints'][0], 'pt') if len(features1['keypoints']) > 0 else False\n            \n            if is_keypoint_object:\n                # Handle KeyPoint objects\n                src_pts = np.float32([features1['keypoints'][m.queryIdx].pt for m in matches])\n                dst_pts = np.float32([features2['keypoints'][m.trainIdx].pt for m in matches])\n            else:\n                # Handle raw keypoints\n                src_pts = np.float32([features1['keypoints'][m.queryIdx] for m in matches])\n                dst_pts = np.float32([features2['keypoints'][m.trainIdx] for m in matches])\n            \n            # Apply strong geometric verification\n            # 1. Find fundamental matrix with RANSAC\n            F, mask_f = cv2.findFundamentalMat(\n                src_pts, dst_pts, \n                method=cv2.FM_RANSAC, \n                ransacReprojThreshold=self.matching_params['ransac_threshold'],\n                confidence=0.999,\n                maxIters=5000\n            )\n            \n            # If fundamental matrix estimation fails, try homography (for planar scenes)\n            if F is None or F.shape != (3, 3):\n                H, mask_h = cv2.findHomography(\n                    src_pts, dst_pts, \n                    method=cv2.RANSAC, \n                    ransacReprojThreshold=3.0,\n                    confidence=0.99,\n                    maxIters=2000\n                )\n                \n                if H is None:\n                    return [], 0, False\n                    \n                inliers = np.squeeze(mask_h.astype(bool))\n                inlier_count = np.sum(inliers)\n                \n                # Only accept homography if high inlier ratio\n                if inlier_count < self.matching_params['min_inliers'] or inlier_count / len(matches) < self.matching_params['min_inlier_ratio']:\n                    return [], 0, False\n                    \n                # Filter matches based on homography inliers\n                filtered_matches = [matches[i] for i in range(len(matches)) if inliers[i]]\n                \n                return filtered_matches, inlier_count, True\n            \n            # Filter matches based on fundamental matrix inliers\n            inliers = np.squeeze(mask_f.astype(bool))\n            inlier_count = np.sum(inliers)\n            \n            # Strict consistency check - high inlier count AND high inlier ratio\n            is_consistent = inlier_count >= self.matching_params['min_inliers'] and inlier_count / len(matches) >= self.matching_params['min_inlier_ratio']\n            \n            if not is_consistent:\n                return [], 0, False\n            \n            # Second verification: Calculate essential matrix and decompose to R,t\n            if self.matching_params.get('geometric_verification', True):\n                dims1 = features1['dimensions']\n                focal1 = max(dims1) / 2\n                pp1 = (dims1[1]/2, dims1[0]/2)\n                \n                # Use points only from fundamental matrix inliers\n                src_pts_inliers = src_pts[inliers]\n                dst_pts_inliers = dst_pts[inliers]\n                \n                # Compute essential matrix\n                E, mask_e = cv2.findEssentialMat(\n                    src_pts_inliers, dst_pts_inliers,\n                    focal=focal1, pp=pp1,\n                    method=cv2.RANSAC,\n                    prob=0.999,\n                    threshold=1.0\n                )\n                \n                if E is None or E.shape != (3, 3):\n                    # Essential matrix failed, but fundamental was good\n                    # Use fundamental matrix results\n                    filtered_matches = [matches[i] for i in range(len(matches)) if inliers[i]]\n                    return filtered_matches, inlier_count, True\n                \n                # Try to recover pose to ensure geometric consistency\n                retval, R, t, mask_pose = cv2.recoverPose(E, src_pts_inliers, dst_pts_inliers, focal=focal1, pp=pp1)\n                \n                # Final filtering: combine all verification steps\n                pose_inliers = np.squeeze(mask_pose.astype(bool))\n                final_inlier_count = np.sum(pose_inliers)\n                \n                # Ensure we have enough inliers after all verification steps\n                final_consistent = final_inlier_count >= self.matching_params['min_inliers']\n                \n                if final_consistent:\n                    # Use matches that passed the fundamental matrix test\n                    filtered_matches = [matches[i] for i in range(len(matches)) if inliers[i]]\n                    return filtered_matches, final_inlier_count, True\n            \n            # If not using second verification or it failed\n            filtered_matches = [matches[i] for i in range(len(matches)) if inliers[i]]\n            return filtered_matches, inlier_count, True\n        \n        except Exception as e:\n            # Handle errors in matching\n            print(f\"Error in geometric verification: {e}\")\n            traceback.print_exc()\n            return [], 0, False\n\n# Initialize the multi-backend matcher\nprint(\"\\nInitializing Multi-Backend Feature Matcher...\")\nmulti_backend_matcher = MultiBackendMatcher(FEATURE_PARAMS, MATCHING_PARAMS)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T06:44:22.986231Z","iopub.execute_input":"2025-04-04T06:44:22.986602Z","iopub.status.idle":"2025-04-04T06:44:23.357991Z","shell.execute_reply.started":"2025-04-04T06:44:22.986572Z","shell.execute_reply":"2025-04-04T06:44:23.357116Z"}},"outputs":[{"name":"stdout","text":"\nInitializing Multi-Backend Feature Matcher...\nInitializing Custom DISK feature extractor...\nDISK feature extractor initialized successfully\nLightGlue with DISK features is available\nInitializing Custom SIFT feature extractor...\nSIFT feature extractor initialized successfully\nLightGlue with SIFT features is available\nLoaded SuperPoint model\nLoading SuperPoint model from /kaggle/working/superglue_models/weights/superpoint_v1.pth\nSuperPoint model loaded successfully\nLoaded SuperGlue model (\"outdoor\" weights)\nLoading SuperGlue model from /kaggle/working/superglue_models/weights/outdoor.pth\nSuperGlue model loaded successfully\nSuperPoint/SuperGlue is available\nTraditional SIFT is available as fallback\nUsing lightglue_disk as primary backend\nAvailable backends (in order): lightglue_disk, lightglue_sift, superglue, sift\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"## Multi-Backend Feature Extraction Pipeline","metadata":{}},{"cell_type":"code","source":"def process_dataset_features_multi_backend(dataset_path, multi_backend_matcher, max_workers=4):\n    \"\"\"Process all images in a dataset using the multi-backend feature extractor.\n    \n    Args:\n        dataset_path: Path to the dataset directory\n        multi_backend_matcher: Multi-backend matcher instance\n        max_workers: Maximum number of parallel workers\n        \n    Returns:\n        Dictionary mapping image names to their features\n    \"\"\"\n    features_dict = {}\n    \n    # Get all image files (only .png for this dataset)\n    image_files = list(dataset_path.glob('*.png'))\n    \n    # Define a worker function for parallel processing\n    def process_image(img_path):\n        features = multi_backend_matcher.extract(img_path)\n        if features is not None and len(features.get('keypoints', [])) > 0:\n            return img_path.name, features\n        return None, None\n    \n    # Process images in parallel\n    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n        futures = [executor.submit(process_image, img_path) for img_path in image_files]\n        \n        for future in tqdm(as_completed(futures), total=len(futures), desc=\"Extracting features\"):\n            img_name, features = future.result()\n            if img_name is not None:\n                features_dict[img_name] = features\n    \n    # Free memory after feature extraction\n    free_memory()\n    \n    # Print feature extraction statistics\n    backends_used = {}\n    for img_name, features in features_dict.items():\n        backend = features.get('backend', 'unknown')\n        backends_used[backend] = backends_used.get(backend, 0) + 1\n    \n    print(f\"Extracted features from {len(features_dict)} images\")\n    for backend, count in backends_used.items():\n        print(f\"  {backend}: {count} images ({count/len(features_dict)*100:.1f}%)\")\n    \n    return features_dict\n\ndef build_match_graph_multi_backend(features_dict, multi_backend_matcher, batch_size=50):\n    \"\"\"Build a graph representing image matches using multi-backend matching.\n    \n    Args:\n        features_dict: Dictionary of image features\n        multi_backend_matcher: Multi-backend matcher instance\n        batch_size: Number of edges to process in each batch\n        \n    Returns:\n        NetworkX graph where nodes are images and edges represent matches\n    \"\"\"\n    # Create graph\n    G = nx.Graph()\n    \n    # Add nodes for each image\n    for img_name in features_dict.keys():\n        G.add_node(img_name)\n    \n    # Create all image pairs for matching\n    image_names = list(features_dict.keys())\n    n = len(image_names)\n    \n    if n <= 1:  # Only proceed if we have at least 2 images\n        return G\n    \n    # Create all pairs for processing\n    pairs = [(i, j) for i in range(n) for j in range(i+1, n)]\n    total_pairs = len(pairs)\n    \n    # Process in batches to avoid memory issues\n    for batch_start in tqdm(range(0, total_pairs, batch_size), desc=\"Building match graph\"):\n        batch_end = min(batch_start + batch_size, total_pairs)\n        batch_pairs = pairs[batch_start:batch_end]\n        \n        for i, j in batch_pairs:\n            img1_name = image_names[i]\n            img2_name = image_names[j]\n            \n            features1 = features_dict[img1_name]\n            features2 = features_dict[img2_name]\n            \n            # Match features and verify\n            matches, inlier_count, is_consistent = multi_backend_matcher.match(features1, features2)\n            \n            # Add edge if geometrically consistent\n            if is_consistent:\n                G.add_edge(img1_name, img2_name, weight=inlier_count, matches=matches)\n        \n        # Free memory after each batch\n        if batch_end % (batch_size * 5) == 0:\n            free_memory()\n    \n    return G\n\ndef process_test_dataset_multi_backend(dataset_path, dataset_name, training_structure, test_dataset_info, multi_backend_matcher):\n    \"\"\"Process a test dataset using multi-backend matching.\n    \n    Args:\n        dataset_path: Path to test dataset\n        dataset_name: Name of the dataset\n        training_structure: Structure information from training data\n        test_dataset_info: Additional analysis info for test datasets\n        multi_backend_matcher: Multi-backend matcher instance\n        \n    Returns:\n        Dictionary with scenes, outliers, and poses\n    \"\"\"\n    print(f\"  Processing dataset '{dataset_name}'...\")\n    \n    # Extract features from test dataset\n    print(\"  Extracting features...\")\n    features_dict = process_dataset_features_multi_backend(dataset_path, multi_backend_matcher)\n    \n    if not features_dict:\n        print(f\"  Warning: No features extracted from {dataset_name}\")\n        return {\n            'scenes': [],\n            'outliers': [img.name for img in dataset_path.glob('*.png')],\n            'poses': {}\n        }\n    \n    # Build match graph\n    print(\"  Building match graph...\")\n    match_graph = build_match_graph_multi_backend(features_dict, multi_backend_matcher)\n    \n    print(f\"  Match graph has {match_graph.number_of_nodes()} nodes and {match_graph.number_of_edges()} edges\")\n    \n    # Get expected scene count from test_dataset_info if available\n    expected_scene_count = None\n    if dataset_name in test_dataset_info:\n        expected_scene_count = test_dataset_info[dataset_name].get('expected_scene_count')\n        print(f\"  Using expected scene count from analysis: {expected_scene_count}\")\n    \n    # Cluster into scenes\n    print(f\"  Clustering scenes...\")\n    try:\n        if HAS_TORCH_GEOMETRIC and False:  # Temporarily disabled torch_geometric\n            print(\"  Using GNN-based clustering...\")\n            scenes, outliers = cluster_scenes_with_deep_features(match_graph, expected_scene_count, device)\n        else:\n            print(\"  Using traditional clustering...\")\n            scenes, outliers = cluster_scenes_high_accuracy(match_graph, expected_scene_count)\n    except Exception as e:\n        print(f\"  Clustering failed: {e}\")\n        traceback.print_exc()\n        print(\"  Falling back to spectral clustering\")\n        scenes, outliers = cluster_scenes_high_accuracy(match_graph, expected_scene_count)\n    \n    print(f\"  Identified {len(scenes)} scenes and {len(outliers)} outliers\")\n    \n    # Estimate poses for each scene\n    all_poses = {}\n    \n    for i, scene in enumerate(scenes):\n        print(f\"  Estimating poses for scene {i+1} ({len(scene)} images)...\")\n        scene_poses = estimate_poses_with_bundle_adjustment(scene, features_dict)\n        \n        # Count registered images\n        registered_count = sum(1 for _, (R, T) in scene_poses.items() if R is not None and T is not None)\n        print(f\"  Registered {registered_count}/{len(scene)} images in scene {i+1}\")\n        \n        all_poses.update(scene_poses)\n        \n        # Free memory\n        free_memory()\n    \n    # Help garbage collection\n    del features_dict\n    del match_graph\n    free_memory()\n    \n    return {\n        'scenes': scenes,\n        'outliers': outliers,\n        'poses': all_poses\n    }\n\n# Test the extraction pipeline with a small sample of images\ndef test_feature_extraction_pipeline(multi_backend_matcher, test_dir=TEST_DIR, max_images=5):\n    \"\"\"Test the feature extraction pipeline on a few images.\"\"\"\n    print(\"\\nTesting feature extraction pipeline...\")\n    \n    # Find test datasets\n    test_datasets = list(Path(test_dir).glob('*'))\n    if not test_datasets:\n        print(\"No test datasets found\")\n        return\n    \n    # Select first dataset\n    test_dataset = test_datasets[0]\n    print(f\"Using test dataset: {test_dataset.name}\")\n    \n    # Get some sample images\n    image_files = list(test_dataset.glob('*.png'))\n    if not image_files:\n        print(\"No images found in test dataset\")\n        return\n    \n    # Limit to a few images\n    sample_images = image_files[:min(max_images, len(image_files))]\n    \n    # Extract features from each image\n    for img_path in sample_images:\n        print(f\"\\nExtracting features from {img_path.name}...\")\n        features = multi_backend_matcher.extract(img_path)\n        \n        if features is not None:\n            backend = features.get('backend', 'unknown')\n            keypoint_count = len(features['keypoints'])\n            descriptor_shape = features['descriptors'].shape if hasattr(features['descriptors'], 'shape') else None\n            \n            print(f\"  Successfully extracted {keypoint_count} features using {backend}\")\n            print(f\"  Descriptor shape: {descriptor_shape}\")\n        else:\n            print(f\"  Failed to extract features\")\n    \n    # Try to match some pairs\n    if len(sample_images) >= 2:\n        img1_path = sample_images[0]\n        img2_path = sample_images[1]\n        \n        print(f\"\\nMatching features between {img1_path.name} and {img2_path.name}...\")\n        features1 = multi_backend_matcher.extract(img1_path)\n        features2 = multi_backend_matcher.extract(img2_path)\n        \n        if features1 is not None and features2 is not None:\n            matches, inlier_count, is_consistent = multi_backend_matcher.match(features1, features2)\n            \n            if is_consistent:\n                print(f\"  Images are geometrically consistent with {inlier_count} inliers\")\n            else:\n                print(f\"  Images are not geometrically consistent\")\n        else:\n            print(f\"  Could not match features (extraction failed)\")\n\n# Test the pipeline\ntest_feature_extraction_pipeline(multi_backend_matcher)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T06:59:01.758136Z","iopub.execute_input":"2025-04-04T06:59:01.758494Z","iopub.status.idle":"2025-04-04T06:59:03.144764Z","shell.execute_reply.started":"2025-04-04T06:59:01.758467Z","shell.execute_reply":"2025-04-04T06:59:03.143971Z"}},"outputs":[{"name":"stdout","text":"\nTesting feature extraction pipeline...\nUsing test dataset: ETs\n\nExtracting features from another_et_another_et004.png...\nDISK extractor response format unexpected, parsing manually...\nCannot parse DISK response: <class 'list'>\nExtracted 1757 SIFT features from another_et_another_et004.png\nExtracted features from another_et_another_et004.png using lightglue_sift backend\n  Successfully extracted 1757 features using lightglue_sift\n  Descriptor shape: (1757, 128)\n\nExtracting features from outliers_out_et003.png...\nDISK extractor response format unexpected, parsing manually...\nCannot parse DISK response: <class 'list'>\nExtracted 1557 SIFT features from outliers_out_et003.png\nExtracted features from outliers_out_et003.png using lightglue_sift backend\n  Successfully extracted 1557 features using lightglue_sift\n  Descriptor shape: (1557, 128)\n\nExtracting features from another_et_another_et006.png...\nDISK extractor response format unexpected, parsing manually...\nCannot parse DISK response: <class 'list'>\nExtracted 1587 SIFT features from another_et_another_et006.png\nExtracted features from another_et_another_et006.png using lightglue_sift backend\n  Successfully extracted 1587 features using lightglue_sift\n  Descriptor shape: (1587, 128)\n\nExtracting features from et_et004.png...\nDISK extractor response format unexpected, parsing manually...\nCannot parse DISK response: <class 'list'>\nExtracted 2135 SIFT features from et_et004.png\nExtracted features from et_et004.png using lightglue_sift backend\n  Successfully extracted 2135 features using lightglue_sift\n  Descriptor shape: (2135, 128)\n\nExtracting features from et_et002.png...\nDISK extractor response format unexpected, parsing manually...\nCannot parse DISK response: <class 'list'>\nExtracted 1355 SIFT features from et_et002.png\nExtracted features from et_et002.png using lightglue_sift backend\n  Successfully extracted 1355 features using lightglue_sift\n  Descriptor shape: (1355, 128)\n\nMatching features between another_et_another_et004.png and outliers_out_et003.png...\nDISK extractor response format unexpected, parsing manually...\nCannot parse DISK response: <class 'list'>\nExtracted 1757 SIFT features from another_et_another_et004.png\nExtracted features from another_et_another_et004.png using lightglue_sift backend\nDISK extractor response format unexpected, parsing manually...\nCannot parse DISK response: <class 'list'>\nExtracted 1557 SIFT features from outliers_out_et003.png\nExtracted features from outliers_out_et003.png using lightglue_sift backend\nMatching descriptors with shapes: (128, 1757) and (128, 1557)\nError in matching descriptors: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 1557 is different from 1757)\n  Images are not geometrically consistent\n","output_type":"stream"},{"name":"stderr","text":"Traceback (most recent call last):\n  File \"<ipython-input-15-9c89ef6c86d6>\", line 234, in _match_descriptors\n    similarity = desc1_norm @ desc2_norm.T\nValueError: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 1557 is different from 1757)\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"## Scene Clustering Algorithms","metadata":{}},{"cell_type":"code","source":"def cluster_scenes_high_accuracy(match_graph, expected_scene_count=None):\n    \"\"\"Cluster scenes using a high accuracy approach that combines spectral clustering,\n    edge weight thresholds, and component analysis.\n    \n    Args:\n        match_graph: NetworkX graph where nodes are images and edges represent matches\n        expected_scene_count: Optional expected number of scenes\n        \n    Returns:\n        Tuple of (scenes, outliers) where scenes is a list of image sets and outliers is a list of images\n    \"\"\"\n    print(f\"Graph has {match_graph.number_of_nodes()} nodes and {match_graph.number_of_edges()} edges\")\n    \n    # Return empty result if the graph is too small\n    if match_graph.number_of_nodes() <= 1:\n        return [], list(match_graph.nodes())\n    \n    # Handle case with no edges (all outliers)\n    if match_graph.number_of_edges() == 0:\n        return [], list(match_graph.nodes())\n    \n    # Create a filtered graph with only strong edges\n    strong_graph = nx.Graph()\n    strong_graph.add_nodes_from(match_graph.nodes())\n    \n    # Determine strength threshold adaptively\n    weights = [data['weight'] for _, _, data in match_graph.edges(data=True)]\n    if not weights:\n        return [], list(match_graph.nodes())\n    \n    weight_threshold = max(20, np.percentile(weights, 80))\n    print(f\"Using edge weight threshold of {weight_threshold}\")\n    \n    # Add edges that meet the threshold\n    for u, v, data in match_graph.edges(data=True):\n        if data['weight'] >= weight_threshold:\n            strong_graph.add_edge(u, v, **data)\n    \n    # First try: Get connected components from strong graph\n    connected_components = list(nx.connected_components(strong_graph))\n    \n    # If we have a reasonable number of components, use them directly\n    if expected_scene_count is not None and abs(len(connected_components) - expected_scene_count) <= 2:\n        print(f\"Using {len(connected_components)} connected components as scenes\")\n        large_components = []\n        min_scene_size = 3  # Minimum images to consider a valid scene\n        \n        for comp in connected_components:\n            if len(comp) >= min_scene_size:\n                large_components.append(list(comp))\n        \n        # All nodes not in large components are outliers\n        all_scene_nodes = set()\n        for comp in large_components:\n            all_scene_nodes.update(comp)\n            \n        outliers = [node for node in match_graph.nodes() if node not in all_scene_nodes]\n        \n        return large_components, outliers\n    \n    # Second try: Try spectral clustering if connected components isn't ideal\n    if expected_scene_count is None:\n        # Estimate number of clusters if not provided\n        # Use heuristic: aim for scenes with 10-30 images based on training data\n        avg_images_per_scene = 15\n        k = max(2, match_graph.number_of_nodes() // avg_images_per_scene)\n        print(f\"Estimated number of scenes: {k}\")\n    else:\n        k = expected_scene_count\n        print(f\"Using expected scene count: {k}\")\n    \n    try:\n        # Create a weighted adjacency matrix from the match graph\n        nodes = list(match_graph.nodes())\n        node_idx = {node: i for i, node in enumerate(nodes)}\n        \n        # Create weighted adjacency matrix\n        n = len(nodes)\n        adj_matrix = np.zeros((n, n))\n        \n        for u, v, data in match_graph.edges(data=True):\n            i, j = node_idx[u], node_idx[v]\n            weight = data['weight']\n            # Higher weights mean stronger connections\n            adj_matrix[i, j] = weight\n            adj_matrix[j, i] = weight\n        \n        # Normalize adjacency matrix for better clustering\n        row_sums = adj_matrix.sum(axis=1)\n        row_sums[row_sums == 0] = 1  # Avoid division by zero\n        adj_matrix_norm = adj_matrix / row_sums[:, np.newaxis]\n        \n        # Try spectral clustering\n        spectral = SpectralClustering(\n            n_clusters=k,\n            affinity='precomputed',\n            random_state=42,\n            assign_labels='discretize',\n            n_init=10\n        )\n        \n        # Fit the model and get cluster labels\n        cluster_labels = spectral.fit_predict(adj_matrix_norm)\n        \n        # Organize nodes by cluster\n        clusters = [[] for _ in range(k)]\n        for i, label in enumerate(cluster_labels):\n            clusters[label].append(nodes[i])\n        \n        # Filter out clusters that are too small (likely outliers)\n        min_cluster_size = 3\n        large_clusters = [cluster for cluster in clusters if len(cluster) >= min_cluster_size]\n        \n        # Collect outliers (small clusters and isolated nodes)\n        outlier_set = set()\n        \n        # Add isolated nodes from match_graph\n        for node in match_graph.nodes():\n            if match_graph.degree(node) == 0:\n                outlier_set.add(node)\n                \n        # Add nodes from small clusters\n        for cluster in clusters:\n            if len(cluster) < min_cluster_size:\n                outlier_set.update(cluster)\n        \n        # Final validation: verify clusters are connected\n        verified_clusters = []\n        \n        for cluster in large_clusters:\n            # Create a subgraph for this cluster\n            subg = match_graph.subgraph(cluster)\n            \n            # Get connected components in the subgraph\n            subg_components = list(nx.connected_components(subg))\n            \n            # Add each component as a separate cluster\n            for comp in subg_components:\n                if len(comp) >= min_cluster_size:\n                    verified_clusters.append(list(comp))\n                else:\n                    # Components that are too small become outliers\n                    outlier_set.update(comp)\n        \n        outliers = list(outlier_set)\n        print(f\"Spectral clustering found {len(verified_clusters)} scenes and {len(outliers)} outliers\")\n        \n        return verified_clusters, outliers\n        \n    except Exception as e:\n        print(f\"Spectral clustering failed: {e}\")\n        traceback.print_exc()\n        \n        # Fallback to DBSCAN if spectral clustering fails\n        print(\"Falling back to DBSCAN clustering\")\n        \n        # Try DBSCAN clustering\n        adjacency_list = []\n        for u, v, data in match_graph.edges(data=True):\n            i, j = node_idx[u], node_idx[v]\n            weight = 1.0 / (data['weight'] + 1e-5)  # Convert to distance (lower is better)\n            adjacency_list.append((i, j, weight))\n            adjacency_list.append((j, i, weight))\n        \n        # Create sparse matrix\n        sparse_matrix = lil_matrix((n, n))\n        for i, j, weight in adjacency_list:\n            sparse_matrix[i, j] = weight\n        \n        # Use DBSCAN for clustering\n        eps = 0.5  # Distance threshold\n        min_samples = 3  # Minimum cluster size\n        \n        db = DBSCAN(eps=eps, min_samples=min_samples, metric='precomputed')\n        db_labels = db.fit_predict(sparse_matrix.tocsr())\n        \n        # Organize nodes by cluster\n        db_clusters = {}\n        for i, label in enumerate(db_labels):\n            if label != -1:  # -1 indicates noise points\n                if label not in db_clusters:\n                    db_clusters[label] = []\n                db_clusters[label].append(nodes[i])\n        \n        # Get outliers (noise points)\n        db_outliers = [nodes[i] for i, label in enumerate(db_labels) if label == -1]\n        \n        # Convert to list of lists format\n        db_clusters_list = list(db_clusters.values())\n        \n        print(f\"DBSCAN found {len(db_clusters_list)} scenes and {len(db_outliers)} outliers\")\n        \n        return db_clusters_list, db_outliers\n\n# If torch_geometric is available, implement the GNN-based clustering approach\nif HAS_TORCH_GEOMETRIC:\n    # Define a GNN Model for node classification\n    class GCNSceneClassifier(nn.Module):\n        \"\"\"Graph Convolutional Network for scene classification.\"\"\"\n        \n        def __init__(self, num_node_features, hidden_dim=64, num_classes=10):\n            \"\"\"Initialize GCN for scene classification.\n            \n            Args:\n                num_node_features: Number of input features per node\n                hidden_dim: Hidden dimension size\n                num_classes: Number of output classes\n            \"\"\"\n            super(GCNSceneClassifier, self).__init__()\n            self.conv1 = GCNConv(num_node_features, hidden_dim)\n            self.conv2 = GCNConv(hidden_dim, hidden_dim)\n            self.conv3 = GCNConv(hidden_dim, num_classes)\n            \n            self.dropout = nn.Dropout(0.3)\n            self.relu = nn.ReLU()\n        \n        def forward(self, x, edge_index, edge_weight=None):\n            \"\"\"Forward pass through the GCN.\n            \n            Args:\n                x: Node features tensor\n                edge_index: Graph connectivity in COO format\n                edge_weight: Edge weights\n                \n            Returns:\n                Node embeddings tensor\n            \"\"\"\n            # First GCN layer\n            x = self.conv1(x, edge_index, edge_weight)\n            x = self.relu(x)\n            x = self.dropout(x)\n            \n            # Second GCN layer\n            x = self.conv2(x, edge_index, edge_weight)\n            x = self.relu(x)\n            x = self.dropout(x)\n            \n            # Final layer\n            x = self.conv3(x, edge_index, edge_weight)\n            \n            return x\n    \n    def cluster_scenes_with_deep_features(match_graph, expected_scene_count=None, device=None):\n        \"\"\"Cluster scenes using a Graph Neural Network approach.\n        \n        Args:\n            match_graph: NetworkX graph where nodes are images and edges represent matches\n            expected_scene_count: Optional expected number of scenes\n            device: Torch device\n            \n        Returns:\n            Tuple of (scenes, outliers) where scenes is a list of image sets and outliers is a list of images\n        \"\"\"\n        if device is None:\n            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        \n        print(f\"Using GNN-based scene clustering on device: {device}\")\n        \n        # Return empty result if the graph is too small\n        if match_graph.number_of_nodes() <= 1:\n            return [], list(match_graph.nodes())\n        \n        # Handle case with no edges (all outliers)\n        if match_graph.number_of_edges() == 0:\n            return [], list(match_graph.nodes())\n        \n        try:\n            # Create node features\n            nodes = list(match_graph.nodes())\n            node_idx = {node: i for i, node in enumerate(nodes)}\n            \n            # Create node features matrix\n            num_nodes = len(nodes)\n            \n            # Use simple features like node degree and edge weights\n            node_degrees = np.array([match_graph.degree(node) for node in nodes])\n            node_weighted_degrees = np.array([\n                sum(data['weight'] for _, _, data in match_graph.edges(node, data=True)) \n                for node in nodes\n            ])\n            \n            # Normalize features\n            node_degrees_norm = node_degrees / (np.max(node_degrees) + 1e-8)\n            node_weighted_degrees_norm = node_weighted_degrees / (np.max(node_weighted_degrees) + 1e-8)\n            \n            # Concatenate features\n            node_features = np.column_stack([\n                node_degrees_norm.reshape(-1, 1),\n                node_weighted_degrees_norm.reshape(-1, 1)\n            ])\n            \n            # Create edge index and weights\n            edge_index = []\n            edge_weights = []\n            \n            for u, v, data in match_graph.edges(data=True):\n                # Convert to indices\n                i, j = node_idx[u], node_idx[v]\n                \n                # Add edge in both directions\n                edge_index.append([i, j])\n                edge_index.append([j, i])\n                \n                # Add edge weights\n                weight = data['weight']\n                edge_weights.append(weight)\n                edge_weights.append(weight)\n            \n            # Convert to PyTorch tensors\n            edge_index = torch.tensor(edge_index, dtype=torch.long).t().to(device)\n            edge_weights = torch.tensor(edge_weights, dtype=torch.float).to(device)\n            node_features = torch.tensor(node_features, dtype=torch.float).to(device)\n            \n            # Determine number of clusters\n            if expected_scene_count is None:\n                # Estimate number of clusters if not provided\n                avg_images_per_scene = 15\n                k = max(2, len(nodes) // avg_images_per_scene)\n                print(f\"Estimated number of scenes: {k}\")\n            else:\n                k = expected_scene_count\n                print(f\"Using expected scene count: {k}\")\n            \n            # Create PyTorch Geometric Data object\n            data = Data(\n                x=node_features,\n                edge_index=edge_index,\n                edge_attr=edge_weights\n            )\n            \n            # Train the GNN model to generate node embeddings\n            model = GCNSceneClassifier(\n                num_node_features=node_features.shape[1],\n                hidden_dim=64,\n                num_classes=k\n            ).to(device)\n            \n            # Set model to evaluation mode (no need for actual training in this case)\n            model.eval()\n            \n            # Generate node embeddings\n            with torch.no_grad():\n                node_embeddings = model(data.x, data.edge_index, data.edge_attr)\n            \n            # Convert embeddings to numpy for clustering\n            embeddings_np = node_embeddings.cpu().numpy()\n            \n            # Apply K-means clustering on the embeddings\n            from sklearn.cluster import KMeans\n            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n            cluster_labels = kmeans.fit_predict(embeddings_np)\n            \n            # Organize nodes by cluster\n            clusters = [[] for _ in range(k)]\n            for i, label in enumerate(cluster_labels):\n                clusters[label].append(nodes[i])\n            \n            # Filter out clusters that are too small (likely outliers)\n            min_cluster_size = 3\n            large_clusters = [cluster for cluster in clusters if len(cluster) >= min_cluster_size]\n            \n            # Identify outliers\n            outlier_set = set()\n            \n            # Add nodes from small clusters\n            for cluster in clusters:\n                if len(cluster) < min_cluster_size:\n                    outlier_set.update(cluster)\n            \n            # Final validation: verify clusters are connected\n            verified_clusters = []\n            \n            for cluster in large_clusters:\n                # Create a subgraph for this cluster\n                subg = match_graph.subgraph(cluster)\n                \n                # Get connected components in the subgraph\n                subg_components = list(nx.connected_components(subg))\n                \n                # Add each component as a separate cluster\n                for comp in subg_components:\n                    if len(comp) >= min_cluster_size:\n                        verified_clusters.append(list(comp))\n                    else:\n                        # Components that are too small become outliers\n                        outlier_set.update(comp)\n            \n            outliers = list(outlier_set)\n            print(f\"GNN-based clustering found {len(verified_clusters)} scenes and {len(outliers)} outliers\")\n            \n            return verified_clusters, outliers\n            \n        except Exception as e:\n            print(f\"GNN-based clustering failed: {e}\")\n            traceback.print_exc()\n            \n            # Fall back to high-accuracy clustering\n            print(\"Falling back to traditional clustering\")\n            return cluster_scenes_high_accuracy(match_graph, expected_scene_count)\n\n# Test the clustering functions\ndef test_scene_clustering():\n    \"\"\"Test function to verify the scene clustering algorithms.\"\"\"\n    print(\"\\nTesting scene clustering algorithms...\")\n    \n    # Create a test graph with two distinct clusters\n    G = nx.Graph()\n    \n    # Add two clusters\n    # Cluster 1: images 1-10\n    for i in range(1, 11):\n        for j in range(i+1, 11):\n            G.add_edge(f\"img{i}.png\", f\"img{j}.png\", weight=100)\n    \n    # Cluster 2: images 11-20\n    for i in range(11, 21):\n        for j in range(i+1, 21):\n            G.add_edge(f\"img{i}.png\", f\"img{j}.png\", weight=100)\n    \n    # Add a few weak connections between clusters\n    G.add_edge(\"img5.png\", \"img15.png\", weight=10)\n    G.add_edge(\"img8.png\", \"img12.png\", weight=15)\n    \n    # Add some outlier nodes with weak connections\n    G.add_node(\"outlier1.png\")\n    G.add_node(\"outlier2.png\")\n    G.add_edge(\"outlier3.png\", \"img3.png\", weight=5)\n    G.add_edge(\"outlier4.png\", \"img18.png\", weight=5)\n    \n    # Try clustering with unknown scene count\n    print(\"\\nClustering with unknown scene count:\")\n    scenes, outliers = cluster_scenes_high_accuracy(G)\n    \n    print(f\"Identified {len(scenes)} scenes and {len(outliers)} outliers\")\n    for i, scene in enumerate(scenes):\n        print(f\"Scene {i+1}: {len(scene)} images\")\n    \n    # Try clustering with known scene count\n    print(\"\\nClustering with known scene count (2):\")\n    scenes, outliers = cluster_scenes_high_accuracy(G, 2)\n    \n    print(f\"Identified {len(scenes)} scenes and {len(outliers)} outliers\")\n    for i, scene in enumerate(scenes):\n        print(f\"Scene {i+1}: {len(scene)} images\")\n    \n    # Test with GNN-based clustering if available\n    if HAS_TORCH_GEOMETRIC:\n        print(\"\\nTesting GNN-based clustering with known scene count (2):\")\n        scenes, outliers = cluster_scenes_with_deep_features(G, 2, device)\n        \n        print(f\"Identified {len(scenes)} scenes and {len(outliers)} outliers\")\n        for i, scene in enumerate(scenes):\n            print(f\"Scene {i+1}: {len(scene)} images\")\n\n# Test scene clustering\ntest_scene_clustering()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T07:03:04.283296Z","iopub.execute_input":"2025-04-04T07:03:04.283637Z","iopub.status.idle":"2025-04-04T07:03:04.452893Z","shell.execute_reply.started":"2025-04-04T07:03:04.283610Z","shell.execute_reply":"2025-04-04T07:03:04.452100Z"}},"outputs":[{"name":"stdout","text":"\nTesting scene clustering algorithms...\n\nClustering with unknown scene count:\nGraph has 24 nodes and 94 edges\nUsing edge weight threshold of 100.0\nEstimated number of scenes: 2\nSpectral clustering found 2 scenes and 2 outliers\nIdentified 2 scenes and 2 outliers\nScene 1: 11 images\nScene 2: 11 images\n\nClustering with known scene count (2):\nGraph has 24 nodes and 94 edges\nUsing edge weight threshold of 100.0\nUsing expected scene count: 2\nSpectral clustering found 2 scenes and 2 outliers\nIdentified 2 scenes and 2 outliers\nScene 1: 11 images\nScene 2: 11 images\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"## Pose Estimation with Bundle Adjustment","metadata":{}},{"cell_type":"code","source":"def estimate_relative_pose(features1, features2):\n    \"\"\"Estimate relative pose between two cameras.\n    \n    Args:\n        features1: Features from the first image\n        features2: Features from the second image\n        \n    Returns:\n        Tuple of (E, R, T) where E is essential matrix, R is rotation, T is translation\n    \"\"\"\n    try:\n        # Extract and match keypoints\n        is_keypoint_object = hasattr(features1['keypoints'][0], 'pt') if len(features1['keypoints']) > 0 else False\n        \n        if is_keypoint_object:\n            # CV2 keypoint objects\n            kp1 = np.float32([kp.pt for kp in features1['keypoints']])\n            kp2 = np.float32([kp.pt for kp in features2['keypoints']])\n        else:\n            # Raw keypoint arrays\n            kp1 = np.float32(features1['keypoints'])\n            kp2 = np.float32(features2['keypoints'])\n        \n        # Get image dimensions\n        dims1 = features1['dimensions']\n        dims2 = features2['dimensions']\n        \n        # Estimate focal length and principal point\n        focal1 = max(dims1[:2]) / 2\n        focal2 = max(dims2[:2]) / 2\n        \n        pp1 = (dims1[1]/2, dims1[0]/2) if len(dims1) >= 2 else (dims1[0]/2, dims1[0]/2)\n        pp2 = (dims2[1]/2, dims2[0]/2) if len(dims2) >= 2 else (dims2[0]/2, dims2[0]/2)\n        \n        # Match features directly\n        desc1 = features1['descriptors']\n        desc2 = features2['descriptors']\n        \n        # Fix descriptor orientation if necessary\n        if desc1.shape[0] == 256 and desc1.shape[1] != 256:\n            # Transpose from (D×N) to (N×D)\n            desc1 = desc1.T\n            desc2 = desc2.T\n        \n        # Normalize descriptors for matching\n        desc1_norm = desc1 / (np.linalg.norm(desc1, axis=1, keepdims=True) + 1e-8)\n        desc2_norm = desc2 / (np.linalg.norm(desc2, axis=1, keepdims=True) + 1e-8)\n        \n        # Compute distance matrix\n        distances = 1.0 - desc1_norm @ desc2_norm.T\n        \n        # Apply ratio test for matching\n        matches = []\n        for i in range(distances.shape[0]):\n            dist = distances[i, :]\n            idx = np.argsort(dist)\n            \n            # Get best and second best matches\n            best_idx = idx[0]\n            second_best_idx = idx[1] if len(idx) > 1 else best_idx\n            \n            # Apply ratio test\n            if dist[best_idx] < 0.8 * dist[second_best_idx]:\n                matches.append(cv2.DMatch(i, best_idx, dist[best_idx]))\n        \n        # Extract matched points\n        if len(matches) < 8:\n            return None, None, None\n        \n        src_pts = np.float32([kp1[m.queryIdx] for m in matches])\n        dst_pts = np.float32([kp2[m.trainIdx] for m in matches])\n        \n        # Compute essential matrix\n        E, mask = cv2.findEssentialMat(\n            src_pts, dst_pts, \n            focal=focal1, pp=pp1,\n            method=cv2.RANSAC, \n            prob=0.999, \n            threshold=1.0\n        )\n        \n        if E is None or E.shape != (3, 3):\n            return None, None, None\n        \n        # Recover relative pose\n        _, R, T, mask = cv2.recoverPose(E, src_pts, dst_pts, focal=focal1, pp=pp1)\n        \n        return E, R, T\n    except Exception as e:\n        print(f\"Error estimating relative pose: {e}\")\n        traceback.print_exc()\n        return None, None, None\n\ndef estimate_poses_with_bundle_adjustment(scene_images, features_dict):\n    \"\"\"Estimate camera poses for a scene using robust SfM and bundle adjustment.\n    \n    Args:\n        scene_images: List of image names in the scene\n        features_dict: Dictionary of image features\n        \n    Returns:\n        Dictionary mapping image names to (R, T) poses\n    \"\"\"\n    # Return empty result if we have too few images\n    if len(scene_images) < 2:\n        return {img: (None, None) for img in scene_images}\n    \n    # Build match graph for this scene\n    scene_graph = nx.Graph()\n    for img in scene_images:\n        scene_graph.add_node(img)\n    \n    # Add edges for pairs with valid features\n    for i, img1 in enumerate(scene_images):\n        if img1 not in features_dict:\n            continue\n            \n        for j in range(i + 1, len(scene_images)):\n            img2 = scene_images[j]\n            if img2 not in features_dict:\n                continue\n                \n            features1 = features_dict[img1]\n            features2 = features_dict[img2]\n            \n            # Skip if features are invalid\n            if (features1 is None or features2 is None or \n                'keypoints' not in features1 or 'keypoints' not in features2 or\n                len(features1['keypoints']) == 0 or len(features2['keypoints']) == 0):\n                continue\n            \n            # Determine backend\n            backend1 = features1.get('backend', 'unknown')\n            backend2 = features2.get('backend', 'unknown')\n            \n            # Skip if backends are incompatible\n            if backend1 != backend2 and not (backend1.startswith('lightglue') and backend2.startswith('lightglue')):\n                # Can only match features with same backend or both lightglue\n                continue\n            \n            # Check if match already stored\n            if scene_graph.has_edge(img1, img2):\n                continue\n                \n            # Estimate relative pose\n            E, R, T = estimate_relative_pose(features1, features2)\n            \n            # Add edge if geometrically consistent\n            if R is not None and T is not None:\n                scene_graph.add_edge(img1, img2, E=E, R=R, T=T)\n    \n    # Check if we have enough edges to reconstruct\n    if scene_graph.number_of_edges() < len(scene_images) - 1:\n        print(f\"Warning: Not enough connected edges for robust reconstruction ({scene_graph.number_of_edges()} < {len(scene_images) - 1})\")\n    \n    # Use spanning tree for initial pose estimates\n    poses = {}\n    \n    # Find maximum spanning tree based on number of inliers\n    mst = nx.maximum_spanning_tree(scene_graph)\n    \n    # Choose first camera as reference\n    reference_camera = scene_images[0]\n    poses[reference_camera] = (np.eye(3), np.zeros((3, 1)))\n    \n    # Perform breadth-first traversal to propagate poses\n    traversal_queue = [reference_camera]\n    visited = set([reference_camera])\n    \n    while traversal_queue:\n        current = traversal_queue.pop(0)\n        \n        # Process neighbors\n        for neighbor in mst.neighbors(current):\n            if neighbor in visited:\n                continue\n                \n            # Mark as visited and add to queue\n            visited.add(neighbor)\n            traversal_queue.append(neighbor)\n            \n            # Get relative pose\n            if mst.has_edge(current, neighbor):\n                # current -> neighbor edge\n                R_rel = mst[current][neighbor]['R']\n                T_rel = mst[current][neighbor]['T']\n                \n                # Get current camera pose\n                R_curr, T_curr = poses[current]\n                \n                # Compute absolute pose of neighbor\n                R_abs = R_rel @ R_curr\n                T_abs = R_rel @ T_curr + T_rel\n                \n                # Store pose\n                poses[neighbor] = (R_abs, T_abs)\n            else:\n                # neighbor -> current edge (need to invert)\n                R_rel = mst[neighbor][current]['R']\n                T_rel = mst[neighbor][current]['T']\n                \n                # Invert relative pose\n                R_rel_inv = R_rel.T\n                T_rel_inv = -R_rel_inv @ T_rel\n                \n                # Get current camera pose\n                R_curr, T_curr = poses[current]\n                \n                # Compute absolute pose of neighbor\n                R_abs = R_rel_inv @ R_curr\n                T_abs = R_rel_inv @ T_curr + T_rel_inv\n                \n                # Store pose\n                poses[neighbor] = (R_abs, T_abs)\n    \n    # Add missing images with null poses\n    for img in scene_images:\n        if img not in poses:\n            poses[img] = (None, None)\n    \n    # TODO: Add bundle adjustment for further refinement\n    # This would be done using sparse bundle adjustment\n    # For now, we return the poses from spanning tree\n    \n    return poses\n\ndef estimate_poses_batch(batched_scenes, features_dict):\n    \"\"\"Estimate poses for batched scenes.\n    \n    Args:\n        batched_scenes: List of scenes, where each scene is a list of image names\n        features_dict: Dictionary of image features\n        \n    Returns:\n        Dictionary mapping image names to (R, T) poses\n    \"\"\"\n    all_poses = {}\n    \n    # Process each scene\n    for i, scene in enumerate(batched_scenes):\n        print(f\"Processing scene {i+1}/{len(batched_scenes)} ({len(scene)} images)\")\n        scene_poses = estimate_poses_with_bundle_adjustment(scene, features_dict)\n        all_poses.update(scene_poses)\n    \n    return all_poses\n\ndef test_pose_estimation():\n    \"\"\"Test function for pose estimation.\"\"\"\n    print(\"\\nTesting pose estimation...\")\n    \n    # Create dummy features for testing\n    features_dict = {}\n    \n    # Create 5 images in a sequence\n    for i in range(5):\n        img_name = f\"img{i}.png\"\n        \n        # Create dummy features\n        keypoints = np.random.rand(100, 2)\n        descriptors = np.random.rand(100, 128)\n        \n        features_dict[img_name] = {\n            'keypoints': keypoints,\n            'descriptors': descriptors,\n            'dimensions': (480, 640),\n            'backend': 'test'\n        }\n    \n    # Dummy scene\n    scene = [f\"img{i}.png\" for i in range(5)]\n    \n    # Test pose estimation\n    poses = estimate_poses_with_bundle_adjustment(scene, features_dict)\n    \n    # Print results\n    print(f\"Estimated poses for {len(poses)} images\")\n    registered_count = sum(1 for _, (R, T) in poses.items() if R is not None and T is not None)\n    print(f\"Successfully registered {registered_count}/{len(scene)} images\")\n\n# Test pose estimation\ntest_pose_estimation()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T07:03:58.272375Z","iopub.execute_input":"2025-04-04T07:03:58.272677Z","iopub.status.idle":"2025-04-04T07:03:58.316553Z","shell.execute_reply.started":"2025-04-04T07:03:58.272653Z","shell.execute_reply":"2025-04-04T07:03:58.315741Z"}},"outputs":[{"name":"stdout","text":"\nTesting pose estimation...\nWarning: Not enough connected edges for robust reconstruction (0 < 4)\nEstimated poses for 5 images\nSuccessfully registered 1/5 images\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"## Main Processing Pipeline","metadata":{}},{"cell_type":"code","source":"def format_and_save_submission(results, output_path):\n    \"\"\"Format results and save to submission file.\n    \n    Args:\n        results: Dictionary mapping dataset names to scene assignments and poses\n        output_path: Path to save submission file\n    \"\"\"\n    # Create output dataframe\n    submission = []\n    \n    for dataset_name, result in results.items():\n        scenes = result['scenes']\n        outliers = result['outliers']\n        poses = result['poses']\n        \n        # Add each assigned scene to submission\n        for scene_idx, scene in enumerate(scenes):\n            for image in scene:\n                R, T = poses.get(image, (None, None))\n                \n                if R is not None and T is not None:\n                    # Format rotation matrix as string\n                    R_str = ';'.join([str(float(r)) for r in R.flatten()])\n                    \n                    # Format translation vector as string\n                    T_str = ';'.join([str(float(t)) for t in T.flatten()])\n                else:\n                    # Image is part of a scene but not registered\n                    R_str = ';'.join(['nan'] * 9)\n                    T_str = ';'.join(['nan'] * 3)\n                \n                # Add to submission\n                submission.append({\n                    'dataset': dataset_name,\n                    'scene': f'cluster{scene_idx+1}',\n                    'image': image,\n                    'rotation_matrix': R_str,\n                    'translation_vector': T_str\n                })\n        \n        # Add outliers to submission\n        for image in outliers:\n            # Use nan values for outliers\n            R_str = ';'.join(['nan'] * 9)\n            T_str = ';'.join(['nan'] * 3)\n            \n            # Add to submission\n            submission.append({\n                'dataset': dataset_name,\n                'scene': 'outliers',\n                'image': image,\n                'rotation_matrix': R_str,\n                'translation_vector': T_str\n            })\n    \n    # Convert to dataframe and save\n    df = pd.DataFrame(submission)\n    \n    # Make sure all required columns are present\n    required_columns = ['dataset', 'scene', 'image', 'rotation_matrix', 'translation_vector']\n    for col in required_columns:\n        if col not in df.columns:\n            df[col] = 'nan'\n    \n    # Ensure columns are in the right order\n    df = df[required_columns]\n    \n    # Save to CSV\n    df.to_csv(output_path, index=False)\n    print(f\"Saved submission to {output_path}\")\n    \n    # Print submission statistics\n    total_images = len(df)\n    outliers = len(df[df['scene'] == 'outliers'])\n    \n    print(f\"Submission statistics:\")\n    print(f\"  Total images: {total_images}\")\n    print(f\"  Outliers: {outliers} ({outliers/total_images*100:.1f}%)\")\n    print(f\"  Registered images: {len(df[df['rotation_matrix'] != ';'.join(['nan']*9)])}\")\n    \n    # Count unique scenes\n    scene_counts = df.groupby('dataset')['scene'].nunique()\n    print(f\"  Average scenes per dataset: {scene_counts.mean():.2f}\")\n    \n    return df\n\ndef process_all_test_datasets(test_dir, multi_backend_matcher, training_structure, test_dataset_info):\n    \"\"\"Process all test datasets.\n    \n    Args:\n        test_dir: Path to test directory\n        multi_backend_matcher: Multi-backend matcher instance\n        training_structure: Training structure information\n        test_dataset_info: Test dataset analysis info\n        \n    Returns:\n        Dictionary with results for all datasets\n    \"\"\"\n    # Find all test datasets\n    test_datasets = {p.name: p for p in Path(test_dir).glob('*') if p.is_dir()}\n    \n    if not test_datasets:\n        print(\"No test datasets found!\")\n        return {}\n    \n    print(f\"Found {len(test_datasets)} test datasets\")\n    \n    # Process each dataset\n    results = {}\n    \n    for dataset_name, dataset_path in test_datasets.items():\n        print(f\"\\nProcessing test dataset '{dataset_name}'...\")\n        \n        # Skip non-directory items\n        if not dataset_path.is_dir():\n            continue\n            \n        # Process dataset\n        result = process_test_dataset_multi_backend(\n            dataset_path, \n            dataset_name, \n            training_structure, \n            test_dataset_info, \n            multi_backend_matcher\n        )\n        \n        # Store result\n        results[dataset_name] = result\n        \n        # Free memory\n        free_memory()\n    \n    return results\n\ndef run_full_pipeline():\n    \"\"\"Run the full processing pipeline.\"\"\"\n    print(\"\\nStarting full processing pipeline...\")\n    \n    # Initialize the multi-backend matcher\n    print(\"Initializing multi-backend matcher...\")\n    matcher = MultiBackendMatcher(FEATURE_PARAMS, MATCHING_PARAMS)\n    \n    # Process all test datasets\n    results = process_all_test_datasets(\n        TEST_DIR,\n        matcher,\n        training_structure,\n        test_dataset_info\n    )\n    \n    # Generate submission file\n    print(\"\\nGenerating submission file...\")\n    submission_df = format_and_save_submission(results, OUTPUT_FILE)\n    \n    print(\"\\nProcessing pipeline complete!\")\n    return submission_df\n\ndef fix_multi_backend_matcher():\n    \"\"\"Fix the multi-backend matcher to address descriptor orientation issues.\"\"\"\n    print(\"\\nFixing MultiBackendMatcher._match_descriptors method...\")\n    \n    # Monkey patch the _match_descriptors method to handle descriptor orientation issues\n    def fixed_match_descriptors(self, desc1, desc2, backend1=None, backend2=None):\n        \"\"\"Match descriptors using cosine similarity and ratio test.\n        This is a safer generic method that works with any descriptor format.\n        \n        Args:\n            desc1, desc2: Feature descriptors as numpy arrays\n            backend1, backend2: Optional backend names for debugging\n            \n        Returns:\n            List of cv2.DMatch objects\n        \"\"\"\n        try:\n            # Print original descriptor shapes\n            print(f\"Original descriptor shapes - desc1: {desc1.shape}, desc2: {desc2.shape}\")\n            \n            # Fix descriptor orientation to ensure they are in (N, D) format\n            # This is crucial for proper matrix multiplication later\n            \n            # Handle SuperPoint descriptors (256, N)\n            if backend1 == 'superglue' or backend2 == 'superglue':\n                # SuperPoint descriptors are typically (256, N)\n                if desc1.shape[0] == 256 and desc1.shape[1] != 256:\n                    # Transpose from (256, N) to (N, 256)\n                    desc1 = desc1.T\n                if desc2.shape[0] == 256 and desc2.shape[1] != 256:\n                    # Transpose from (256, N) to (N, 256)\n                    desc2 = desc2.T\n                print(f\"Transposed SuperPoint descriptors to: {desc1.shape} and {desc2.shape}\")\n            \n            # Handle SIFT descriptors which might be transposed\n            elif backend1 == 'lightglue_sift' or backend2 == 'lightglue_sift':\n                # Check if descriptors are in (D, N) format instead of (N, D)\n                if desc1.shape[0] < desc1.shape[1]:\n                    # Already in (N, D) format\n                    pass\n                else:\n                    # Transpose from (D, N) to (N, D)\n                    desc1 = desc1.T\n                    desc2 = desc2.T\n                    print(f\"Transposed SIFT descriptors to: {desc1.shape} and {desc2.shape}\")\n            \n            # Handle 1D descriptors (reshape to 2D)\n            elif hasattr(desc1, 'shape') and len(desc1.shape) == 1:\n                desc1 = desc1.reshape(1, -1)\n                desc2 = desc2.reshape(1, -1)\n                print(f\"Reshaped 1D descriptors to: {desc1.shape} and {desc2.shape}\")\n            \n            # Handle list descriptors\n            elif isinstance(desc1, list):\n                desc1 = np.array(desc1)\n                desc2 = np.array(desc2)\n                print(f\"Converted list descriptors to arrays: {desc1.shape} and {desc2.shape}\")\n            \n            # Ensure we have 2D arrays with features as rows\n            if len(desc1.shape) != 2 or len(desc2.shape) != 2:\n                print(f\"Unexpected descriptor shapes: {desc1.shape} and {desc2.shape}\")\n                # Try to reshape if possible\n                if len(desc1.shape) > 2:\n                    desc1 = desc1.reshape(desc1.shape[0], -1)\n                if len(desc2.shape) > 2:\n                    desc2 = desc2.reshape(desc2.shape[0], -1)\n            \n            # Print final descriptor shapes\n            print(f\"Final descriptor shapes for matching - desc1: {desc1.shape}, desc2: {desc2.shape}\")\n            \n            # Normalize descriptors for cosine similarity\n            desc1_norm = np.copy(desc1)\n            desc2_norm = np.copy(desc2)\n            \n            # Normalize rows\n            desc1_norms = np.linalg.norm(desc1_norm, axis=1, keepdims=True) + 1e-10\n            desc2_norms = np.linalg.norm(desc2_norm, axis=1, keepdims=True) + 1e-10\n            \n            desc1_norm = desc1_norm / desc1_norms\n            desc2_norm = desc2_norm / desc2_norms\n            \n            # Create distance matrix (avoid matmul issues)\n            distances = np.zeros((desc1_norm.shape[0], desc2_norm.shape[0]))\n            \n            # Compute pairwise distances manually to avoid matmul issues\n            # This is an alternative to the matrix multiplication approach\n            for i in range(desc1_norm.shape[0]):\n                for j in range(desc2_norm.shape[0]):\n                    # Cosine similarity\n                    dot_product = np.dot(desc1_norm[i], desc2_norm[j])\n                    # Convert to distance (1 - similarity)\n                    distances[i, j] = 1.0 - dot_product\n            \n            # Apply ratio test\n            good_matches = []\n            for i in range(distances.shape[0]):\n                # Get distances for this descriptor\n                dist = distances[i]\n                \n                # Find best match (lowest distance)\n                best_idx = np.argmin(dist)\n                best_dist = dist[best_idx]\n                \n                # Find second best match\n                dist_copy = dist.copy()\n                dist_copy[best_idx] = float('inf')\n                second_best_idx = np.argmin(dist_copy)\n                second_best_dist = dist_copy[second_best_idx]\n                \n                # Apply ratio test (lower ratio means stricter test)\n                if best_dist < 0.8 * second_best_dist:\n                    m = cv2.DMatch()\n                    m.queryIdx = i\n                    m.trainIdx = best_idx\n                    m.distance = best_dist\n                    good_matches.append(m)\n            \n            print(f\"Generic descriptor matcher: Found {len(good_matches)} matches\")\n            return good_matches\n        except Exception as e:\n            print(f\"Error in generic descriptor matching: {e}\")\n            traceback.print_exc()\n            return []\n    \n    # Apply the patch to the MultiBackendMatcher instance\n    MultiBackendMatcher._match_descriptors = fixed_match_descriptors\n    \n    print(\"Matcher fixed successfully!\")\n\n# Fix the multi-backend matcher\nfix_multi_backend_matcher()\n\n# Run a small test of the pipeline\ndef test_pipeline():\n    \"\"\"Run a small test of the pipeline.\"\"\"\n    print(\"\\nRunning small pipeline test...\")\n    \n    # Initialize matcher\n    matcher = MultiBackendMatcher(FEATURE_PARAMS, MATCHING_PARAMS)\n    \n    # Test feature extraction and matching\n    test_feature_extraction_pipeline(matcher, max_images=3)\n    \n    # Test clustering\n    test_scene_clustering()\n    \n    # Test pose estimation\n    test_pose_estimation()\n    \n    print(\"\\nPipeline test complete!\")\n\n# Pipeline test\ntest_pipeline()\n\n# Run full pipeline\nrun_full_pipeline()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T07:05:31.703249Z","iopub.execute_input":"2025-04-04T07:05:31.703600Z","iopub.status.idle":"2025-04-04T07:05:49.523068Z","shell.execute_reply.started":"2025-04-04T07:05:31.703570Z","shell.execute_reply":"2025-04-04T07:05:49.521896Z"}},"outputs":[{"name":"stdout","text":"\nFixing MultiBackendMatcher._match_descriptors method...\nMatcher fixed successfully!\n\nRunning small pipeline test...\nInitializing Custom DISK feature extractor...\nDISK feature extractor initialized successfully\nLightGlue with DISK features is available\nInitializing Custom SIFT feature extractor...\nSIFT feature extractor initialized successfully\nLightGlue with SIFT features is available\nLoaded SuperPoint model\nLoading SuperPoint model from /kaggle/working/superglue_models/weights/superpoint_v1.pth\nSuperPoint model loaded successfully\nLoaded SuperGlue model (\"outdoor\" weights)\nLoading SuperGlue model from /kaggle/working/superglue_models/weights/outdoor.pth\nSuperGlue model loaded successfully\nSuperPoint/SuperGlue is available\nTraditional SIFT is available as fallback\nUsing lightglue_disk as primary backend\nAvailable backends (in order): lightglue_disk, lightglue_sift, superglue, sift\n\nTesting feature extraction pipeline...\nUsing test dataset: ETs\n\nExtracting features from another_et_another_et004.png...\nDISK extractor response format unexpected, parsing manually...\nCannot parse DISK response: <class 'list'>\nExtracted 1757 SIFT features from another_et_another_et004.png\nExtracted features from another_et_another_et004.png using lightglue_sift backend\n  Successfully extracted 1757 features using lightglue_sift\n  Descriptor shape: (1757, 128)\n\nExtracting features from outliers_out_et003.png...\nDISK extractor response format unexpected, parsing manually...\nCannot parse DISK response: <class 'list'>\nExtracted 1557 SIFT features from outliers_out_et003.png\nExtracted features from outliers_out_et003.png using lightglue_sift backend\n  Successfully extracted 1557 features using lightglue_sift\n  Descriptor shape: (1557, 128)\n\nExtracting features from another_et_another_et006.png...\nDISK extractor response format unexpected, parsing manually...\nCannot parse DISK response: <class 'list'>\nExtracted 1587 SIFT features from another_et_another_et006.png\nExtracted features from another_et_another_et006.png using lightglue_sift backend\n  Successfully extracted 1587 features using lightglue_sift\n  Descriptor shape: (1587, 128)\n\nMatching features between another_et_another_et004.png and outliers_out_et003.png...\nDISK extractor response format unexpected, parsing manually...\nCannot parse DISK response: <class 'list'>\nExtracted 1757 SIFT features from another_et_another_et004.png\nExtracted features from another_et_another_et004.png using lightglue_sift backend\nDISK extractor response format unexpected, parsing manually...\nCannot parse DISK response: <class 'list'>\nExtracted 1557 SIFT features from outliers_out_et003.png\nExtracted features from outliers_out_et003.png using lightglue_sift backend\nMatching descriptors with shapes: (128, 1757) and (128, 1557)\nError in matching descriptors: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 1557 is different from 1757)\n  Images are not geometrically consistent\n\nTesting scene clustering algorithms...\n\nClustering with unknown scene count:\nGraph has 24 nodes and 94 edges\nUsing edge weight threshold of 100.0\nEstimated number of scenes: 2\nSpectral clustering found 2 scenes and 2 outliers\nIdentified 2 scenes and 2 outliers\nScene 1: 11 images\nScene 2: 11 images\n\nClustering with known scene count (2):\nGraph has 24 nodes and 94 edges\nUsing edge weight threshold of 100.0\nUsing expected scene count: 2\nSpectral clustering found 2 scenes and 2 outliers\nIdentified 2 scenes and 2 outliers\nScene 1: 11 images\nScene 2: 11 images\n\nTesting pose estimation...\nWarning: Not enough connected edges for robust reconstruction (0 < 4)\nEstimated poses for 5 images\nSuccessfully registered 1/5 images\n\nPipeline test complete!\n\nStarting full processing pipeline...\nInitializing multi-backend matcher...\nInitializing Custom DISK feature extractor...\nDISK feature extractor initialized successfully\nLightGlue with DISK features is available\nInitializing Custom SIFT feature extractor...\nSIFT feature extractor initialized successfully\nLightGlue with SIFT features is available\nLoaded SuperPoint model\nLoading SuperPoint model from /kaggle/working/superglue_models/weights/superpoint_v1.pth\nSuperPoint model loaded successfully\nLoaded SuperGlue model (\"outdoor\" weights)\nLoading SuperGlue model from /kaggle/working/superglue_models/weights/outdoor.pth\n","output_type":"stream"},{"name":"stderr","text":"Traceback (most recent call last):\n  File \"<ipython-input-15-9c89ef6c86d6>\", line 234, in _match_descriptors\n    similarity = desc1_norm @ desc2_norm.T\nValueError: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 1557 is different from 1757)\n","output_type":"stream"},{"name":"stdout","text":"SuperGlue model loaded successfully\nSuperPoint/SuperGlue is available\nTraditional SIFT is available as fallback\nUsing lightglue_disk as primary backend\nAvailable backends (in order): lightglue_disk, lightglue_sift, superglue, sift\nFound 2 test datasets\n\nProcessing test dataset 'ETs'...\n  Processing dataset 'ETs'...\n  Extracting features...\n","output_type":"stream"},{"name":"stderr","text":"Extracting features:   5%|▍         | 1/22 [00:04<01:29,  4.27s/it]","output_type":"stream"},{"name":"stdout","text":"DISK extractor response format unexpected, parsing manually...\nCannot parse DISK response: <class 'list'>\nExtracted 1557 SIFT features from outliers_out_et003.png\nExtracted features from outliers_out_et003.png using lightglue_sift backend\nDISK extractor response format unexpected, parsing manually...\nCannot parse DISK response: <class 'list'>\nExtracted 1757 SIFT features from another_et_another_et004.png\nExtracted features from another_et_another_et004.png using lightglue_sift backend\n","output_type":"stream"},{"name":"stderr","text":"Extracting features:   9%|▉         | 2/22 [00:04<00:42,  2.12s/it]","output_type":"stream"},{"name":"stdout","text":"DISK extractor response format unexpected, parsing manually...\nCannot parse DISK response: <class 'list'>\nExtracted 1202 SIFT features from another_et_another_et009.png\nExtracted features from another_et_another_et009.png using lightglue_sift backend\n","output_type":"stream"},{"name":"stderr","text":"Extracting features:  14%|█▎        | 3/22 [00:05<00:27,  1.47s/it]","output_type":"stream"},{"name":"stdout","text":"DISK extractor response format unexpected, parsing manually...\nCannot parse DISK response: <class 'list'>\nDISK extractor response format unexpected, parsing manually...\nCannot parse DISK response: <class 'list'>\nExtracted 1587 SIFT features from another_et_another_et006.png\nExtracted features from another_et_another_et006.png using lightglue_sift backend\n","output_type":"stream"},{"name":"stderr","text":"Extracting features:  23%|██▎       | 5/22 [00:06<00:11,  1.44it/s]","output_type":"stream"},{"name":"stdout","text":"Extracted 2135 SIFT features from et_et004.png\nExtracted features from et_et004.png using lightglue_sift backend\n","output_type":"stream"},{"name":"stderr","text":"Extracting features:  27%|██▋       | 6/22 [00:08<00:20,  1.28s/it]","output_type":"stream"},{"name":"stdout","text":"DISK extractor response format unexpected, parsing manually...\nCannot parse DISK response: <class 'list'>\nDISK extractor response format unexpected, parsing manually...\nCannot parse DISK response: <class 'list'>\nExtracted 648 SIFT features from outliers_out_et001.png\nExtracted features from outliers_out_et001.png using lightglue_sift backend\nExtracted 1355 SIFT features from et_et002.png\nExtracted features from et_et002.png using lightglue_sift backend\nDISK extractor response format unexpected, parsing manually...DISK extractor response format unexpected, parsing manually...\n\nCannot parse DISK response: <class 'list'>\nCannot parse DISK response: <class 'list'>\n","output_type":"stream"},{"name":"stderr","text":"Extracting features:  41%|████      | 9/22 [00:09<00:08,  1.61it/s]","output_type":"stream"},{"name":"stdout","text":"Extracted 1452 SIFT features from et_et008.png\nExtracted features from et_et008.png using lightglue_sift backend\nExtracted 1701 SIFT features from another_et_another_et008.png\nExtracted features from another_et_another_et008.png using lightglue_sift backend\nDISK extractor response format unexpected, parsing manually...\nCannot parse DISK response: <class 'list'>\nDISK extractor response format unexpected, parsing manually...\nCannot parse DISK response: <class 'list'>\n","output_type":"stream"},{"name":"stderr","text":"Extracting features:  50%|█████     | 11/22 [00:12<00:09,  1.11it/s]","output_type":"stream"},{"name":"stdout","text":"Extracted 1908 SIFT features from another_et_another_et005.png\nExtracted features from another_et_another_et005.png using lightglue_sift backend\nDISK extractor response format unexpected, parsing manually...\nCannot parse DISK response: <class 'list'>\nExtracted 1245 SIFT features from et_et006.png\nExtracted features from et_et006.png using lightglue_sift backend\n","output_type":"stream"},{"name":"stderr","text":"Extracting features:  55%|█████▍    | 12/22 [00:12<00:06,  1.47it/s]","output_type":"stream"},{"name":"stdout","text":"Extracted 1729 SIFT features from another_et_another_et003.png\nExtracted features from another_et_another_et003.png using lightglue_sift backend\nDISK extractor response format unexpected, parsing manually...\nCannot parse DISK response: <class 'list'>\nDISK extractor response format unexpected, parsing manually...\nCannot parse DISK response: <class 'list'>\nExtracted 2034 SIFT features from another_et_another_et002.png\nExtracted features from another_et_another_et002.png using lightglue_sift backend\n","output_type":"stream"},{"name":"stderr","text":"Extracting features:  64%|██████▎   | 14/22 [00:12<00:07,  1.11it/s]","output_type":"stream"},{"name":"stdout","text":"DISK extractor response format unexpected, parsing manually...\nCannot parse DISK response: <class 'list'>\nExtracted 1328 SIFT features from et_et007.png\nExtracted features from et_et007.png using lightglue_sift backend\nExtracted 1679 SIFT features from et_et000.png\nExtracted features from et_et000.png using lightglue_sift backend\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"DISK extractor response format unexpected, parsing manually...\nCannot parse DISK response: <class 'list'>\nDISK extractor response format unexpected, parsing manually...\nCannot parse DISK response: <class 'list'>\nExtracted 1103 SIFT features from another_et_another_et010.png\nExtracted features from another_et_another_et010.png using lightglue_sift backend\nExtracted 1422 SIFT features from et_et001.png\nExtracted features from et_et001.png using lightglue_sift backend\nDISK extractor response format unexpected, parsing manually...\nCannot parse DISK response: <class 'list'>\nDISK extractor response format unexpected, parsing manually...\nCannot parse DISK response: <class 'list'>\nExtracted 1071 SIFT features from et_et005.png\nExtracted features from et_et005.png using lightglue_sift backend\nExtracted 2211 SIFT features from another_et_another_et001.png\nExtracted features from another_et_another_et001.png using lightglue_sift backend\nDISK extractor response format unexpected, parsing manually...\nCannot parse DISK response: <class 'list'>\nDISK extractor response format unexpected, parsing manually...\nCannot parse DISK response: <class 'list'>\nExtracted 1737 SIFT features from another_et_another_et007.png\nExtracted features from another_et_another_et007.png using lightglue_sift backend\nExtracted 1603 SIFT features from et_et003.png\nExtracted features from et_et003.png using lightglue_sift backend\nDISK extractor response format unexpected, parsing manually...\nCannot parse DISK response: <class 'list'>\nExtracted 445 SIFT features from outliers_out_et002.png\nExtracted features from outliers_out_et002.png using lightglue_sift backend\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-06a5bfe0a526>\u001b[0m in \u001b[0;36m<cell line: 313>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[0;31m# Run full pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m \u001b[0mrun_full_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-22-06a5bfe0a526>\u001b[0m in \u001b[0;36mrun_full_pipeline\u001b[0;34m()\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;31m# Process all test datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m     results = process_all_test_datasets(\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0mTEST_DIR\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0mmatcher\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-22-06a5bfe0a526>\u001b[0m in \u001b[0;36mprocess_all_test_datasets\u001b[0;34m(test_dir, multi_backend_matcher, training_structure, test_dataset_info)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;31m# Process dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         result = process_test_dataset_multi_backend(\n\u001b[0m\u001b[1;32m    120\u001b[0m             \u001b[0mdataset_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mdataset_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-18-cbb4218d9f90>\u001b[0m in \u001b[0;36mprocess_test_dataset_multi_backend\u001b[0;34m(dataset_path, dataset_name, training_structure, test_dataset_info, multi_backend_matcher)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;31m# Extract features from test dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"  Extracting features...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0mfeatures_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_dataset_features_multi_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_backend_matcher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfeatures_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-18-cbb4218d9f90>\u001b[0m in \u001b[0;36mprocess_dataset_features_multi_backend\u001b[0;34m(dataset_path, multi_backend_matcher, max_workers)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mfutures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mexecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubmit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimg_path\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimage_files\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mfuture\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mas_completed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfutures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfutures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Extracting features\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0mimg_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mimg_name\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mas_completed\u001b[0;34m(fs, timeout)\u001b[0m\n\u001b[1;32m    243\u001b[0m                             len(pending), total_futures))\n\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    605\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 607\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    608\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":22},{"cell_type":"markdown","source":"## Visualize and Evaluate Results","metadata":{}},{"cell_type":"code","source":"def visualize_scene_clusters(dataset_name, dataset_results, output_dir=None):\n    \"\"\"Visualize scene clusters with sample images.\n    \n    Args:\n        dataset_name: Name of the dataset\n        dataset_results: Results dictionary with scenes and outliers\n        output_dir: Optional directory to save visualizations\n    \"\"\"\n    import matplotlib.pyplot as plt\n    from matplotlib.gridspec import GridSpec\n    \n    scenes = dataset_results['scenes']\n    outliers = dataset_results['outliers']\n    \n    # Create directory for visualizations if needed\n    if output_dir is not None:\n        os.makedirs(output_dir, exist_ok=True)\n    \n    # Set up figure\n    n_scenes = len(scenes)\n    fig = plt.figure(figsize=(15, 5 * (n_scenes + 1)))\n    gs = GridSpec(n_scenes + 1, 1, figure=fig)\n    \n    # Plot scenes\n    for i, scene in enumerate(scenes):\n        ax = fig.add_subplot(gs[i, 0])\n        ax.set_title(f\"Scene {i+1}: {len(scene)} images\")\n        \n        # Display sample images from this scene (up to 5)\n        sample_images = scene[:min(5, len(scene))]\n        \n        # Create a grid for sample images\n        n_samples = len(sample_images)\n        img_grid = GridSpec(1, n_samples, subplot_spec=gs[i, 0])\n        \n        for j, img_name in enumerate(sample_images):\n            img_ax = fig.add_subplot(img_grid[0, j])\n            img_path = Path(TEST_DIR) / dataset_name / img_name\n            \n            try:\n                img = plt.imread(img_path)\n                img_ax.imshow(img)\n                img_ax.set_title(f\"{img_name[:10]}...\")\n                img_ax.axis('off')\n            except Exception as e:\n                img_ax.text(0.5, 0.5, f\"Error: {e}\", ha='center', va='center')\n                img_ax.axis('off')\n    \n    # Plot outliers\n    if outliers:\n        ax = fig.add_subplot(gs[n_scenes, 0])\n        ax.set_title(f\"Outliers: {len(outliers)} images\")\n        \n        # Display sample outlier images (up to 5)\n        sample_outliers = outliers[:min(5, len(outliers))]\n        \n        # Create a grid for sample images\n        n_samples = len(sample_outliers)\n        img_grid = GridSpec(1, n_samples, subplot_spec=gs[n_scenes, 0])\n        \n        for j, img_name in enumerate(sample_outliers):\n            img_ax = fig.add_subplot(img_grid[0, j])\n            img_path = Path(TEST_DIR) / dataset_name / img_name\n            \n            try:\n                img = plt.imread(img_path)\n                img_ax.imshow(img)\n                img_ax.set_title(f\"{img_name[:10]}...\")\n                img_ax.axis('off')\n            except Exception as e:\n                img_ax.text(0.5, 0.5, f\"Error: {e}\", ha='center', va='center')\n                img_ax.axis('off')\n    \n    plt.tight_layout()\n    \n    # Save or display the visualization\n    if output_dir is not None:\n        plt.savefig(os.path.join(output_dir, f\"{dataset_name}_scenes.png\"))\n        plt.close()\n    else:\n        plt.show()\n\ndef visualize_camera_poses(dataset_name, scene_idx, scene_images, poses, output_dir=None):\n    \"\"\"Visualize camera poses in 3D.\n    \n    Args:\n        dataset_name: Name of the dataset\n        scene_idx: Scene index\n        scene_images: List of image names in the scene\n        poses: Dictionary of camera poses\n        output_dir: Optional directory to save visualizations\n    \"\"\"\n    from mpl_toolkits.mplot3d import Axes3D\n    import matplotlib.pyplot as plt\n    import numpy as np\n    \n    # Filter out images without valid poses\n    valid_images = [img for img in scene_images if img in poses and poses[img][0] is not None]\n    \n    if not valid_images:\n        print(f\"No valid poses for {dataset_name}, scene {scene_idx}\")\n        return\n    \n    # Create figure\n    fig = plt.figure(figsize=(10, 8))\n    ax = fig.add_subplot(111, projection='3d')\n    \n    # Plot camera positions\n    positions = []\n    for img in valid_images:\n        R, t = poses[img]\n        positions.append(t)\n    \n    positions = np.array(positions)\n    \n    # Calculate center\n    center = np.mean(positions, axis=0)\n    \n    # Plot camera positions\n    ax.scatter(positions[:, 0], positions[:, 1], positions[:, 2], c='blue', s=50)\n    \n    # Plot camera viewing directions\n    for img in valid_images:\n        R, t = poses[img]\n        \n        # Camera viewing direction (toward -Z in camera coordinates)\n        direction = R.T @ np.array([0, 0, -1])\n        \n        # Scale direction vector for visualization\n        scale = 0.5\n        \n        # Plot viewing direction\n        ax.quiver(t[0], t[1], t[2], \n                  direction[0] * scale, direction[1] * scale, direction[2] * scale, \n                  color='red', arrow_length_ratio=0.1)\n    \n    # Set axis labels\n    ax.set_xlabel('X')\n    ax.set_ylabel('Y')\n    ax.set_zlabel('Z')\n    \n    # Set title\n    ax.set_title(f\"{dataset_name}, Scene {scene_idx}: {len(valid_images)} cameras\")\n    \n    # Center the view on the mean position\n    ax.set_xlim(center[0] - 2, center[0] + 2)\n    ax.set_ylim(center[1] - 2, center[1] + 2)\n    ax.set_zlim(center[2] - 2, center[2] + 2)\n    \n    plt.tight_layout()\n    \n    # Save or display the visualization\n    if output_dir is not None:\n        os.makedirs(output_dir, exist_ok=True)\n        plt.savefig(os.path.join(output_dir, f\"{dataset_name}_scene{scene_idx}_poses.png\"))\n        plt.close()\n    else:\n        plt.show()\n\ndef visualize_results(submission_df, test_dir, output_dir='visualizations'):\n    \"\"\"Visualize the results from the submission file.\n    \n    Args:\n        submission_df: Submission dataframe\n        test_dir: Test directory\n        output_dir: Directory to save visualizations\n    \"\"\"\n    # Create output directory\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # Group results by dataset\n    for dataset in submission_df['dataset'].unique():\n        dataset_df = submission_df[submission_df['dataset'] == dataset]\n        \n        # Extract scenes and outliers\n        scenes = []\n        for scene in dataset_df['scene'].unique():\n            if scene != 'outliers':\n                scene_images = dataset_df[dataset_df['scene'] == scene]['image'].tolist()\n                scenes.append(scene_images)\n        \n        outliers = dataset_df[dataset_df['scene'] == 'outliers']['image'].tolist()\n        \n        # Create dataset results\n        dataset_results = {\n            'scenes': scenes,\n            'outliers': outliers,\n            'poses': {}\n        }\n        \n        # Extract poses\n        for _, row in dataset_df.iterrows():\n            if row['scene'] != 'outliers':\n                # Parse rotation matrix\n                try:\n                    R_str = row['rotation_matrix']\n                    if 'nan' not in R_str:\n                        R = np.array([float(x) for x in R_str.split(';')]).reshape(3, 3)\n                    \n                        # Parse translation vector\n                        T_str = row['translation_vector']\n                        T = np.array([float(x) for x in T_str.split(';')])\n                        \n                        dataset_results['poses'][row['image']] = (R, T)\n                    else:\n                        dataset_results['poses'][row['image']] = (None, None)\n                except Exception:\n                    dataset_results['poses'][row['image']] = (None, None)\n        \n        # Visualize scene clusters\n        visualize_scene_clusters(dataset, dataset_results, output_dir)\n        \n        # Visualize camera poses for each scene\n        for i, scene in enumerate(scenes):\n            visualize_camera_poses(dataset, i+1, scene, dataset_results['poses'], output_dir)\n\n# Add this to the end of the main function to visualize results\ndef visualize_submission_results():\n    \"\"\"Visualize the results from the submission file.\"\"\"\n    if os.path.exists(OUTPUT_FILE):\n        submission_df = pd.read_csv(OUTPUT_FILE)\n        visualize_results(submission_df, TEST_DIR)\n    else:\n        print(\"Submission file not found. Run the pipeline first.\")\n\nif __name__ == \"__main__\":\n    # Execute the main pipeline\n    main()\n    \n    # Visualize results (optional)\n    try:\n        visualize_submission_results()\n    except Exception as e:\n        print(f\"Visualization failed: {e}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-04T05:50:59.184Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}